{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking low dataset\n",
    "\n",
    "* train_test_split\n",
    "* data preprocessing\n",
    "  \n",
    "\n",
    "### Model building and test with mlflow and kfold\n",
    "\n",
    "** Test with 3 dataset. 1 dataset for each models. **\n",
    "- defog - and its models. (search for its best models.)\n",
    "- tdcsfog - and its models. (search for its best models and parameters)\n",
    "- Both dataset combine and its models. (search for its best models and parameters)\n",
    "\n",
    "### Model searching the best parameter\n",
    "\n",
    "### Testing with many data again and Retune\n",
    "\n",
    "### final model with test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# set_defog_dataset = pd.read_parquet('Data/Process/set_defog_data.parquet')\n",
    "# test_defog_dataset = pd.read_parquet('Data/Process/test_defog_data.parquet')\n",
    "# Test_defog_dataset = pd.read_csv('Data/test/defog/02ab235146.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing_the_dataset(df):\n",
    "#     def check_skewness(df):\n",
    "#     # this can check relation between each column\n",
    "#         skew_limit=0.75\n",
    "#         skew_value=df[df.columns].skew()\n",
    "#         #print(skew_value)\n",
    "#         skew_col=skew_value[abs(skew_value)>skew_limit]\n",
    "#         cols=skew_col.index\n",
    "#         return cols\n",
    "\n",
    "#     import random \n",
    "#     random_seed = 54\n",
    "    \n",
    "#     feature_col = ['AccV','AccML','AccAP']\n",
    "#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "\n",
    "#     # make feature and label\n",
    "#     feature_dataset = df[feature_col]\n",
    "#     label_dataset = df[label_col]\n",
    "    \n",
    "#     # check skewness and powertransform\n",
    "#     skew_columns = check_skewness(feature_dataset)\n",
    "#     print(skew_columns)\n",
    "    \n",
    "#     print(\"Power Transform start\")\n",
    "#     from sklearn.preprocessing import PowerTransformer\n",
    "#     pt=PowerTransformer(standardize=False)  \n",
    "#     feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n",
    "    \n",
    "#     print(\"Standardization start\")\n",
    "#     # Change features data to 0 and 1\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     sc=StandardScaler()\n",
    "#     feature_dataset=sc.fit_transform(feature_dataset)\n",
    "    \n",
    "#     print(\"Train test split begin\")\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n",
    "    \n",
    "#     train_feature = np.array(train_feature) \n",
    "#     valid_feature = np.array(valid_feature)\n",
    "#     train_label  = np.array(train_label)\n",
    "#     valid_label = np.array(valid_label)\n",
    "    \n",
    "#     return train_feature, valid_feature, train_label, valid_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AccAP'], dtype='object')\n",
      "Power Transform start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_339776/1180494003.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization start\n",
      "Train test split begin\n"
     ]
    }
   ],
   "source": [
    "# train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_defog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29126467, 3), (7281617, 3), (29126467, 4), (7281617, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Data/defog/train_feature',train_feature)\n",
    "# np.save('Data/defog/valid_feature',valid_feature)\n",
    "# np.save('Data/defog/train_label',train_label)\n",
    "# np.save('Data/defog/valid_label',valid_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bz of low memory, I save the features and label then load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "train_feature = np.load('Data/defog/train_feature.npy')\n",
    "valid_feature = np.load('Data/defog/valid_feature.npy')\n",
    "train_label = np.load('Data/defog/train_label.npy')\n",
    "valid_label = np.load('Data/defog/valid_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29126467, 3), (7281617, 3), (29126467, 4), (7281617, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn model\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Building Model Dict\n",
    "Models = {                         \n",
    "    \"Decision Tree\": DecisionTreeClassifier(),      \n",
    "    \"KNearest\": KNeighborsClassifier(),           \n",
    "    \"Ridge\" : RidgeClassifier(),                  \n",
    "    \"MLP\" : MLPClassifier(),                      \n",
    "    \"Extra_T\" : ExtraTreesClassifier(),\n",
    "    \"R_forest\" : RandomForestClassifier(),\n",
    "    \"XGB\" : xgb.XGBClassifier(),\n",
    "    # \"Catboost\" : CatBoostClassifier()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGB\n",
    "\n",
    "# lgb_train = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1})\n",
    "# lgb_test = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1}, reference=lgb_train)\n",
    "\n",
    "# LGBM = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=False)\n",
    "# LGBM_pred=LGBM.predict(test_nm_features)\n",
    "# LGBM_pred = LGBM_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# !mlflow server --backend-store-uri sqlite:///backend.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(classifier, test_features, test_labels):\n",
    "    \n",
    "    # make prediction\n",
    "    predictions   = classifier.predict(test_features)\n",
    "    \n",
    "    base_score   = classifier.score(test_features,test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    av_precision = average_precision_score(test_labels, predictions)\n",
    "    # Matrix = confusion_matrix(test_labels, predictions)\n",
    "    \n",
    "    # matrix_scores = { \n",
    "    #     \"true negative\"  : Matrix[0][0],\n",
    "    #     \"false positive\" : Matrix[0][1],\n",
    "    #     \"false negative\" : Matrix[1][0],\n",
    "    #     \"true positive \" : Matrix[1][1]\n",
    "    # }\n",
    "    \n",
    "    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "    print(\"Classification report\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(classification_report(test_labels, predictions,target_names=target_names),\"\\n\")\n",
    "    # print(\"Confusion Matrix\")\n",
    "    # print(\"---------------------\",\"\\n\")\n",
    "    # print(f\"{Matrix} \\n\")\n",
    "\n",
    "    print(\"Accuracy Measures\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(\"Base score: \", base_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Avarge Precision: \", av_precision)\n",
    "    \n",
    "    return base_score,accuracy,av_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://127.0.0.1:5000'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Catboost\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcounter\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m{\u001b[39;00mModel_Name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run():\n\u001b[1;32m      8\u001b[0m     \u001b[39m# fit the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     classifier\u001b[39m.\u001b[39;49mfit(train_feature, train_label)\n\u001b[1;32m     10\u001b[0m     \u001b[39m# cross_val_scores = cross_val_score(classifier, train_feature, train_label, cv=5)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m# round_scores = round(cross_val_scores.mean(), 4) * 100\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round_scores , \"% accuracy score\")\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     counter \u001b[39m=\u001b[39m counter \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/catboost/core.py:5128\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5126\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5128\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[1;32m   5129\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[1;32m   5130\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[1;32m   5131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/catboost/core.py:2339\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, PATH_TYPES \u001b[39m+\u001b[39m (Pool,)):\n\u001b[1;32m   2337\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2339\u001b[0m train_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_train_params(\n\u001b[1;32m   2340\u001b[0m     X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, cat_features\u001b[39m=\u001b[39;49mcat_features, text_features\u001b[39m=\u001b[39;49mtext_features, embedding_features\u001b[39m=\u001b[39;49membedding_features,\n\u001b[1;32m   2341\u001b[0m     pairs\u001b[39m=\u001b[39;49mpairs, sample_weight\u001b[39m=\u001b[39;49msample_weight, group_id\u001b[39m=\u001b[39;49mgroup_id, group_weight\u001b[39m=\u001b[39;49mgroup_weight,\n\u001b[1;32m   2342\u001b[0m     subgroup_id\u001b[39m=\u001b[39;49msubgroup_id, pairs_weight\u001b[39m=\u001b[39;49mpairs_weight, baseline\u001b[39m=\u001b[39;49mbaseline, use_best_model\u001b[39m=\u001b[39;49muse_best_model,\n\u001b[1;32m   2343\u001b[0m     eval_set\u001b[39m=\u001b[39;49meval_set, verbose\u001b[39m=\u001b[39;49mverbose, logging_level\u001b[39m=\u001b[39;49mlogging_level, plot\u001b[39m=\u001b[39;49mplot, plot_file\u001b[39m=\u001b[39;49mplot_file,\n\u001b[1;32m   2344\u001b[0m     column_description\u001b[39m=\u001b[39;49mcolumn_description, verbose_eval\u001b[39m=\u001b[39;49mverbose_eval, metric_period\u001b[39m=\u001b[39;49mmetric_period,\n\u001b[1;32m   2345\u001b[0m     silent\u001b[39m=\u001b[39;49msilent, early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, save_snapshot\u001b[39m=\u001b[39;49msave_snapshot,\n\u001b[1;32m   2346\u001b[0m     snapshot_file\u001b[39m=\u001b[39;49msnapshot_file, snapshot_interval\u001b[39m=\u001b[39;49msnapshot_interval, init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[1;32m   2347\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m   2348\u001b[0m )\n\u001b[1;32m   2349\u001b[0m params \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2350\u001b[0m train_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mtrain_pool\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/catboost/core.py:2228\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[1;32m   2224\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mX is empty.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2226\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, Pool)\n\u001b[0;32m-> 2228\u001b[0m params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m _get_loss_function_for_train(\n\u001b[1;32m   2229\u001b[0m     params,\n\u001b[1;32m   2230\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m_estimator_type\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   2231\u001b[0m     train_pool\n\u001b[1;32m   2232\u001b[0m )\n\u001b[1;32m   2234\u001b[0m metric_period, verbose, logging_level \u001b[39m=\u001b[39m _process_verbose(\n\u001b[1;32m   2235\u001b[0m     metric_period, verbose, logging_level, verbose_eval, silent)\n\u001b[1;32m   2237\u001b[0m \u001b[39mif\u001b[39;00m metric_period \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/catboost/core.py:1671\u001b[0m, in \u001b[0;36m_get_loss_function_for_train\u001b[0;34m(params, estimator_type, train_pool)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m'\u001b[39m\u001b[39mloss function has not been specified and cannot be deduced\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1667\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[39m        len(set) is faster than np.unique on Python lists:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[39m         https://bbengfort.github.io/observations/2017/05/02/python-unique-benchmark.html\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1671\u001b[0m     is_multiclass_task \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39;49m(label)) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtarget_border\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params\n\u001b[1;32m   1672\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mMultiClass\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m is_multiclass_task \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mLogloss\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39melif\u001b[39;00m estimator_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mranker\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "counter = 1\n",
    "for Model_Name, classifier in Models.items(): \n",
    "    # with mlflow.start_run(nested=True):\n",
    "    print(f\"{counter}. {Model_Name}\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # fit the model\n",
    "        classifier.fit(train_feature, train_label)\n",
    "        # cross_val_scores = cross_val_score(classifier, train_feature, train_label, cv=5)\n",
    "        # round_scores = round(cross_val_scores.mean(), 4) * 100\n",
    "        # print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round_scores , \"% accuracy score\")\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        base_score,accuracy,av_precision = eval_metrics(classifier,\n",
    "                                                        valid_feature,\n",
    "                                                        valid_label)  \n",
    "        \n",
    "        mlflow.log_param(\"Model\"           , Model_Name)\n",
    "        mlflow.log_metric(\"base_score\"     , base_score)\n",
    "        mlflow.log_metric(\"accuracy\"       , accuracy)\n",
    "        mlflow.log_metric(\"av_precision\"   , av_precision)\n",
    "        # mlflow.log_metric(\"cross_val_score\", cross_val_scores)\n",
    "        # mlflow.log_metric(\"round_score\", round_scores)\n",
    "        \n",
    "        signature = infer_signature(valid_feature, classifier.predict(valid_feature))\n",
    "        if av_precision > 0.95 :\n",
    "            mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n",
    "            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n",
    "        else :\n",
    "            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n",
    "        \n",
    "        print(\"________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
