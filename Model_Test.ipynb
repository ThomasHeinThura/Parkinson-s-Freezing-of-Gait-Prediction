{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking low dataset\n",
    "\n",
    "* train_test_split\n",
    "* data preprocessing\n",
    "  \n",
    "\n",
    "### Model building and test with mlflow and kfold\n",
    "\n",
    "** Test with 3 dataset. 1 dataset for each models. **\n",
    "- defog - and its models. (search for its best models.)\n",
    "- tdcsfog - and its models. (search for its best models and parameters)\n",
    "- Both dataset combine and its models. (search for its best models and parameters)\n",
    "\n",
    "### Model searching the best parameter\n",
    "\n",
    "### Testing with many data again and Retune\n",
    "\n",
    "### final model with test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "set_defog_dataset = pd.read_parquet('Data/Process/set_defog_data.parquet')\n",
    "# test_defog_dataset = pd.read_parquet('Data/Process/test_defog_data.parquet')\n",
    "# Test_defog_dataset = pd.read_csv('Data/test/defog/02ab235146.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_the_dataset(df):\n",
    "    def check_skewness(df):\n",
    "    # this can check relation between each column\n",
    "        skew_limit=0.75\n",
    "        skew_value=df[df.columns].skew()\n",
    "        #print(skew_value)\n",
    "        skew_col=skew_value[abs(skew_value)>skew_limit]\n",
    "        cols=skew_col.index\n",
    "        return cols\n",
    "\n",
    "    import random \n",
    "    random_seed = 54\n",
    "    \n",
    "    feature_col = ['AccV','AccML','AccAP']\n",
    "    label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "\n",
    "    # make feature and label\n",
    "    feature_dataset = df[feature_col]\n",
    "    label_dataset = df[label_col]\n",
    "    \n",
    "    # check skewness and powertransform\n",
    "    skew_columns = check_skewness(feature_dataset)\n",
    "    print(skew_columns)\n",
    "    \n",
    "    print(\"Power Transform start\")\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    pt=PowerTransformer(standardize=False)  \n",
    "    feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n",
    "    \n",
    "    print(\"Standardization start\")\n",
    "    # Change features data to 0 and 1\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc=StandardScaler()\n",
    "    feature_dataset=sc.fit_transform(feature_dataset)\n",
    "    \n",
    "    print(\"Train test split begin\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n",
    "    \n",
    "    return train_feature, valid_feature, train_label, valid_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AccAP'], dtype='object')\n",
      "Power Transform start\n"
     ]
    }
   ],
   "source": [
    "train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_defog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn model\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "# import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Building Model Dict\n",
    "Models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),                    \n",
    "    \"Support Vector Classifier\": SVC(),                             \n",
    "    \"Decision Tree\": DecisionTreeClassifier(),           \n",
    "    \"KNearest\": KNeighborsClassifier(),                \n",
    "    \"GaussianNB\" : GaussianNB(),                                    \n",
    "    \"LDA\" : LinearDiscriminantAnalysis(),                            \n",
    "    \"Ridge\" : RidgeClassifier(),                                      \n",
    "    \"QDA\" : QuadraticDiscriminantAnalysis(),                        \n",
    "    \"Bagging\" : BaggingClassifier(),                                \n",
    "    \"MLP\" : MLPClassifier(),                                        \n",
    "    \"LSVC\" : LinearSVC(),                                           \n",
    "    \"BernoulliNB\" : BernoulliNB(),                                  \n",
    "    \"Passive_AC\" : PassiveAggressiveClassifier(),                   \n",
    "    \"SGB\"     : GradientBoostingClassifier(),\n",
    "    \"Adaboost\" : AdaBoostClassifier(),\n",
    "    \"Extra_T\" : ExtraTreesClassifier(),\n",
    "    \"R_forest\" : RandomForestClassifier(),\n",
    "    \"XGB\" : xgb.XGBClassifier(),\n",
    "    \"Catboost\" : CatBoostClassifier()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGB\n",
    "\n",
    "# lgb_train = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1})\n",
    "# lgb_test = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1}, reference=lgb_train)\n",
    "\n",
    "# LGBM = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=False)\n",
    "# LGBM_pred=LGBM.predict(test_nm_features)\n",
    "# LGBM_pred = LGBM_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "!mlflow server --backend-store-uri sqlite:///backend.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(classifier, test_features, test_labels):\n",
    "    \n",
    "    # make prediction\n",
    "    predictions   = classifier.predict(test_features)\n",
    "    \n",
    "    base_score   = classifier.score(test_features,test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    av_precision = average_precision_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    f1score = f1_score(test_labels, predictions)\n",
    "    Matrix = confusion_matrix(test_labels, predictions)\n",
    "    \n",
    "    matrix_scores = { \n",
    "        \"true negative\"  : Matrix[0][0],\n",
    "        \"false positive\" : Matrix[0][1],\n",
    "        \"false negative\" : Matrix[1][0],\n",
    "        \"true positive \" : Matrix[1][1]\n",
    "    }\n",
    "    \n",
    "    target_names = ['0','1']\n",
    "    print(\"Classification report\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(classification_report(test_labels, predictions,target_names=target_names),\"\\n\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(f\"{Matrix} \\n\")\n",
    "\n",
    "    print(\"Accuracy Measures\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(\"Base score: \", base_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1score)\n",
    "    \n",
    "    return base_score,accuracy,precision,recall,f1score,matrix_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # This function train all the Dataset and all models\n",
    "# def Train_all_Model():\n",
    "\n",
    "#     Dataset_scores = {}\n",
    "\n",
    "#     for Name, values in Meta_Dataset.items():\n",
    "#         print(f\"The {Name} Dataset is using.........\")\n",
    "#         i = 1\n",
    "#         Model_scores = {} #Model = Socre\n",
    "#         for Model_Name, classifier in Models.items():\n",
    "#             print(f\"{i}. {Model_Name}\")\n",
    "#             classifier.fit(values[0], values[1])\n",
    "#             score = calculate_socre(classifier=classifier,\n",
    "#                                     test_features=values[2],\n",
    "#                                     test_labels=values[3])\n",
    "#             i = i+1\n",
    "#             print(f\"{score}\")\n",
    "#             Model_scores[Model_Name] = score\n",
    "#         print(\"________________________________________\")\n",
    "#         Dataset_scores[Name] = Model_scores\n",
    "#     return Dataset_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "for Model_Name, classifier in Models.items(): \n",
    "    # with mlflow.start_run(nested=True):\n",
    "    print(f\"{counter}. {Model_Name}\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # fit the model\n",
    "        classifier.fit(train_feature, train_label)\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        base_score,accuracy,precision,recall,f1score,matrix_scores = eval_metrics(classifier,\n",
    "                                                                                  valid_feature,\n",
    "                                                                                  valid_label)  \n",
    "        \n",
    "        mlflow.log_param(\"Model\"           , Model_Name)\n",
    "        mlflow.log_param(\"Sampling\"        , sampling)\n",
    "        mlflow.log_metric(\"base_score\"     , base_score)\n",
    "        mlflow.log_metric(\"accuracy\"       , accuracy)\n",
    "        mlflow.log_metric(\"av_precision\"   , precision)\n",
    "        mlflow.log_metric(\"recall\"         , recall)\n",
    "        mlflow.log_metric(\"f1\"             , f1score)\n",
    "        mlflow.log_params(matrix_scores)\n",
    "        \n",
    "        signature = infer_signature(test_features, classifier.predict(test_features))\n",
    "        if f1score > 0.95 :\n",
    "            mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n",
    "            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n",
    "        else :\n",
    "            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n",
    "        \n",
    "        print(\"________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
