{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking low dataset\n",
    "\n",
    "* train_test_split\n",
    "* data preprocessing\n",
    "  \n",
    "\n",
    "### Model building and test with mlflow and kfold\n",
    "\n",
    "** Test with 3 dataset. 1 dataset for each models. **\n",
    "- defog - and its models. (search for its best models.)\n",
    "- tdcsfog - and its models. (search for its best models and parameters)\n",
    "- Both dataset combine and its models. (search for its best models and parameters)\n",
    "\n",
    "### Model searching the best parameter\n",
    "\n",
    "### Testing with many data again and Retune\n",
    "\n",
    "### final model with test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# set_defog_dataset = pd.read_parquet('Data/Process/set_defog_data.parquet')\n",
    "# test_defog_dataset = pd.read_parquet('Data/Process/test_defog_data.parquet')\n",
    "# Test_defog_dataset = pd.read_csv('Data/test/defog/02ab235146.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing_the_dataset(df):\n",
    "#     def check_skewness(df):\n",
    "#     # this can check relation between each column\n",
    "#         skew_limit=0.75\n",
    "#         skew_value=df[df.columns].skew()\n",
    "#         #print(skew_value)\n",
    "#         skew_col=skew_value[abs(skew_value)>skew_limit]\n",
    "#         cols=skew_col.index\n",
    "#         return cols\n",
    "\n",
    "#     import random \n",
    "#     random_seed = 54\n",
    "    \n",
    "#     feature_col = ['AccV','AccML','AccAP']\n",
    "#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "\n",
    "#     # make feature and label\n",
    "#     feature_dataset = df[feature_col]\n",
    "#     label_dataset = df[label_col]\n",
    "    \n",
    "#     # check skewness and powertransform\n",
    "#     skew_columns = check_skewness(feature_dataset)\n",
    "#     print(skew_columns)\n",
    "    \n",
    "#     print(\"Power Transform start\")\n",
    "#     from sklearn.preprocessing import PowerTransformer\n",
    "#     pt=PowerTransformer(standardize=False)  \n",
    "#     feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n",
    "    \n",
    "#     print(\"Standardization start\")\n",
    "#     # Change features data to 0 and 1\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     sc=StandardScaler()\n",
    "#     feature_dataset=sc.fit_transform(feature_dataset)\n",
    "    \n",
    "#     print(\"Train test split begin\")\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n",
    "    \n",
    "#     train_feature = np.array(train_feature) \n",
    "#     valid_feature = np.array(valid_feature)\n",
    "#     train_label  = np.array(train_label)\n",
    "#     valid_label = np.array(valid_label)\n",
    "    \n",
    "#     return train_feature, valid_feature, train_label, valid_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_defog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Data/defog/train_feature',train_feature)\n",
    "# np.save('Data/defog/valid_feature',valid_feature)\n",
    "# np.save('Data/defog/train_label',train_label)\n",
    "# np.save('Data/defog/valid_label',valid_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bz of low memory, I save the features and label then load it"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 6,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "train_feature = np.load('Data/defog/train_feature.npy')\n",
    "valid_feature = np.load('Data/defog/valid_feature.npy')\n",
    "train_label = np.load('Data/defog/train_label.npy')\n",
    "valid_label = np.load('Data/defog/valid_label.npy')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 7,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29126467, 3), (7281617, 3), (29126467, 4), (7281617, 4))"
      ]
     },
<<<<<<< HEAD
     "execution_count": 3,
=======
     "execution_count": 7,
>>>>>>> fbbd28a (test xgb)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 8,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn model\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "\n",
    "# import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 9,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Building Model Dict\n",
    "Models = {                         \n",
    "    # \"Decision Tree\": DecisionTreeClassifier(),      \n",
    "    # \"KNearest\": KNeighborsClassifier(),           \n",
    "    # \"Ridge\" : RidgeClassifier(),              #poor result    \n",
    "    # \"MLP\" : MLPClassifier(),                  #poor result              \n",
    "    # \"R_Neighour\" : RadiusNeighborsClassifier(),\n",
    "    # \"Extra_T\" : ExtraTreesClassifier(),\n",
    "    # \"R_forest\" : RandomForestClassifier(),\n",
    "    \"XGB\" : xgb.XGBClassifier(),\n",
    "    # \"Catboost\" : CatBoostClassifier()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 10,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGB\n",
    "\n",
    "# lgb_train = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1})\n",
    "# lgb_test = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1}, reference=lgb_train)\n",
    "\n",
    "# LGBM = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=False)\n",
    "# LGBM_pred=LGBM.predict(test_nm_features)\n",
    "# LGBM_pred = LGBM_pred.round()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 11,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 12,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# !mlflow server --backend-store-uri sqlite:///backend.db"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 13,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(classifier, test_features, test_labels):\n",
    "    \n",
    "    # make prediction\n",
    "    predictions   = classifier.predict(test_features)\n",
    "    \n",
    "    base_score   = classifier.score(test_features,test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    av_precision = average_precision_score(test_labels, predictions)\n",
    "    # Matrix = confusion_matrix(test_labels, predictions)\n",
    "    \n",
    "    # matrix_scores = { \n",
    "    #     \"true negative\"  : Matrix[0][0],\n",
    "    #     \"false positive\" : Matrix[0][1],\n",
    "    #     \"false negative\" : Matrix[1][0],\n",
    "    #     \"true positive \" : Matrix[1][1]\n",
    "    # }\n",
    "    \n",
    "    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "    print(\"Classification report\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(classification_report(test_labels, predictions,target_names=target_names),\"\\n\")\n",
    "    # print(\"Confusion Matrix\")\n",
    "    # print(\"---------------------\",\"\\n\")\n",
    "    # print(f\"{Matrix} \\n\")\n",
    "\n",
    "    print(\"Accuracy Measures\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(\"Base score: \", base_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Avarge Precision: \", av_precision)\n",
    "    \n",
    "    return base_score,accuracy,av_precision"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 14,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://127.0.0.1:5000'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 15,
>>>>>>> fbbd28a (test xgb)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1. R_Neighour\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
=======
      "1. XGB\n",
      "Classification report\n",
      "--------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/miniconda3/envs/main/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       0.92      0.95      0.93   1822100\n",
      "           Turn       0.72      0.53      0.61   1819501\n",
      "        Walking       0.70      0.55      0.61   1819885\n",
      "       All_zero       0.84      0.51      0.64   1820131\n",
      "\n",
      "      micro avg       0.80      0.63      0.71   7281617\n",
      "      macro avg       0.79      0.63      0.70   7281617\n",
      "   weighted avg       0.79      0.63      0.70   7281617\n",
      "    samples avg       0.63      0.63      0.63   7281617\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.6292563863218843\n",
      "Accuracy:  0.6292563863218843\n",
      "Avarge Precision:  0.6075655920483543\n",
      "Because f1 socre is not quality. The model is skip to saving phase.\n",
      "________________________________________\n"
>>>>>>> fbbd28a (test xgb)
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "counter = 1\n",
    "for Model_Name, classifier in Models.items(): \n",
    "    # with mlflow.start_run(nested=True):\n",
    "    print(f\"{counter}. {Model_Name}\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # fit the model\n",
    "        classifier.fit(train_feature, train_label)\n",
    "        # cross_val_scores = cross_val_score(classifier, train_feature, train_label, cv=5)\n",
    "        # round_scores = round(cross_val_scores.mean(), 4) * 100\n",
    "        # print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round_scores , \"% accuracy score\")\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        base_score,accuracy,av_precision = eval_metrics(classifier,\n",
    "                                                        valid_feature,\n",
    "                                                        valid_label)  \n",
    "        \n",
    "        mlflow.log_param(\"Model\"           , Model_Name)\n",
    "        mlflow.log_metric(\"base_score\"     , base_score)\n",
    "        mlflow.log_metric(\"accuracy\"       , accuracy)\n",
    "        mlflow.log_metric(\"av_precision\"   , av_precision)\n",
    "        # mlflow.log_metric(\"cross_val_score\", cross_val_scores)\n",
    "        # mlflow.log_metric(\"round_score\", round_scores)\n",
    "        \n",
    "        signature = infer_signature(valid_feature, classifier.predict(valid_feature))\n",
    "        if av_precision > 0.95 :\n",
    "            mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n",
    "            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n",
    "        else :\n",
    "            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n",
    "        \n",
    "        print(\"________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
