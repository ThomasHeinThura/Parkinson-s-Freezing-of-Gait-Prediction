{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# set_defog_dataset = pd.read_parquet('Data/Process/set_defog_data.parquet')\n",
    "# test_defog_dataset = pd.read_parquet('Data/Process/test_defog_data.parquet')\n",
    "# Test_defog_dataset = pd.read_csv('Data/test/defog/02ab235146.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing_the_dataset(df):\n",
    "#     def check_skewness(df):\n",
    "#     # this can check relation between each column\n",
    "#         skew_limit=0.75\n",
    "#         skew_value=df[df.columns].skew()\n",
    "#         #print(skew_value)\n",
    "#         skew_col=skew_value[abs(skew_value)>skew_limit]\n",
    "#         cols=skew_col.index\n",
    "#         return cols\n",
    "\n",
    "#     import random \n",
    "#     random_seed = 54\n",
    "    \n",
    "#     feature_col = ['AccV','AccML','AccAP']\n",
    "#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "\n",
    "#     # make feature and label\n",
    "#     feature_dataset = df[feature_col]\n",
    "#     label_dataset = df[label_col]\n",
    "    \n",
    "#     # check skewness and powertransform\n",
    "#     skew_columns = check_skewness(feature_dataset)\n",
    "#     print(skew_columns)\n",
    "    \n",
    "#     print(\"Power Transform start\")\n",
    "#     from sklearn.preprocessing import PowerTransformer\n",
    "#     pt=PowerTransformer(standardize=False)  \n",
    "#     feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n",
    "    \n",
    "#     print(\"Standardization start\")\n",
    "#     # Change features data to 0 and 1\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     sc=StandardScaler()\n",
    "#     feature_dataset=sc.fit_transform(feature_dataset)\n",
    "    \n",
    "#     print(\"Train test split begin\")\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n",
    "    \n",
    "#     train_feature = np.array(train_feature) \n",
    "#     valid_feature = np.array(valid_feature)\n",
    "#     train_label  = np.array(train_label)\n",
    "#     valid_label = np.array(valid_label)\n",
    "    \n",
    "#     return train_feature, valid_feature, train_label, valid_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_defog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Data/defog/train_feature',train_feature)\n",
    "# np.save('Data/defog/valid_feature',valid_feature)\n",
    "# np.save('Data/defog/train_label',train_label)\n",
    "# np.save('Data/defog/valid_label',valid_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bz of low memory, I save the features and label then load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "train_feature = np.load('Data/defog/train_feature.npy')\n",
    "valid_feature = np.load('Data/defog/valid_feature.npy')\n",
    "train_label = np.load('Data/defog/train_label.npy')\n",
    "valid_label = np.load('Data/defog/valid_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29126467, 3), (7281617, 3), (29126467, 4), (7281617, 4))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn model\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "\n",
    "# import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Building Model Dict\n",
    "Models = {                         \n",
    "    \"Decision Tree\": DecisionTreeClassifier(),      \n",
    "    # \"KNearest\": KNeighborsClassifier(n_jobs=-1),           \n",
    "    # \"Ridge\" : RidgeClassifier(),              #poor result    \n",
    "    # \"MLP\" : MLPClassifier(),                  #poor result              \n",
    "    # \"R_Neighour\" : RadiusNeighborsClassifier(),\n",
    "    # \"Extra_T\" : ExtraTreesClassifier(),\n",
    "    # \"R_forest\" : RandomForestClassifier(),\n",
    "    # \"XGB\" : xgb.XGBClassifier(),\n",
    "    # \"Catboost\" : CatBoostClassifier()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGB\n",
    "\n",
    "# lgb_train = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1})\n",
    "# lgb_test = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1}, reference=lgb_train)\n",
    "\n",
    "# LGBM = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=False)\n",
    "# LGBM_pred=LGBM.predict(test_nm_features)\n",
    "# LGBM_pred = LGBM_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# !mlflow server --backend-store-uri sqlite:///backend.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(classifier, test_features, test_labels):\n",
    "    \n",
    "    # make prediction\n",
    "    predictions   = classifier.predict(test_features)\n",
    "    \n",
    "    base_score   = classifier.score(test_features,test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    av_precision = average_precision_score(test_labels, predictions)\n",
    "    # Matrix = confusion_matrix(test_labels, predictions)\n",
    "    \n",
    "    # matrix_scores = { \n",
    "    #     \"true negative\"  : Matrix[0][0],\n",
    "    #     \"false positive\" : Matrix[0][1],\n",
    "    #     \"false negative\" : Matrix[1][0],\n",
    "    #     \"true positive \" : Matrix[1][1]\n",
    "    # }\n",
    "    \n",
    "    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "    print(\"Classification report\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(classification_report(test_labels, predictions,target_names=target_names),\"\\n\")\n",
    "    # print(\"Confusion Matrix\")\n",
    "    # print(\"---------------------\",\"\\n\")\n",
    "    # print(f\"{Matrix} \\n\")\n",
    "\n",
    "    print(\"Accuracy Measures\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(\"Base score: \", base_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Avarge Precision: \", av_precision)\n",
    "    \n",
    "    return base_score,accuracy,av_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://127.0.0.1:5000'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Decision Tree\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 89.4 % accuracy score\n",
      "Classification report\n",
      "--------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/miniconda3/envs/main/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       1.00      1.00      1.00   1822100\n",
      "           Turn       0.86      0.88      0.87   1819501\n",
      "        Walking       0.92      0.94      0.93   1819885\n",
      "       All_zero       0.86      0.81      0.83   1820131\n",
      "\n",
      "      micro avg       0.91      0.91      0.91   7281617\n",
      "      macro avg       0.91      0.91      0.91   7281617\n",
      "   weighted avg       0.91      0.91      0.91   7281617\n",
      "    samples avg       0.91      0.91      0.91   7281617\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9089180329039552\n",
      "Accuracy:  0.9089180329039552\n",
      "Avarge Precision:  0.8522690981167873\n",
      "Because f1 socre is not quality. The model is skip to saving phase.\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "counter = 1\n",
    "for Model_Name, classifier in Models.items(): \n",
    "    # with mlflow.start_run(nested=True):\n",
    "    print(f\"{counter}. {Model_Name}\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # fit the model\n",
    "        from joblib import parallel_backend\n",
    "\n",
    "        with parallel_backend('threading', n_jobs=-1):\n",
    "            classifier.fit(train_feature, train_label)\n",
    "        cross_val_scores = cross_val_score(classifier, train_feature, train_label, cv=3)\n",
    "        round_scores = round(cross_val_scores.mean(), 4) * 100\n",
    "        print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round_scores , \"% accuracy score\")\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        base_score,accuracy,av_precision = eval_metrics(classifier,\n",
    "                                                        valid_feature,\n",
    "                                                        valid_label)  \n",
    "        \n",
    "        mlflow.log_param(\"Model\"           , Model_Name)\n",
    "        mlflow.log_metric(\"base_score\"     , base_score)\n",
    "        mlflow.log_metric(\"accuracy\"       , accuracy)\n",
    "        mlflow.log_metric(\"av_precision\"   , av_precision)\n",
    "        mlflow.log_metric(\"round_score\", round_scores)\n",
    "        \n",
    "        signature = infer_signature(valid_feature, classifier.predict(valid_feature))\n",
    "        if av_precision > 0.95 :\n",
    "            mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n",
    "            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n",
    "        else :\n",
    "            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n",
    "        \n",
    "        print(\"________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "              \"min_samples_leaf\": list(range(5,7,1))},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Grid search for finding the most optimal hyperparameters\n",
    "def gridsearch(classifier, params):\n",
    "    grid_classifier = GridSearchCV(classifier, params)\n",
    "    grid_classifier.fit(train_feature, train_label)\n",
    "    best_classifier = grid_classifier.best_estimator_\n",
    "    return best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores after applying GridSearch:-\n",
      "Decision Tree: 89.396\n"
     ]
    }
   ],
   "source": [
    "print('Cross-Validation Scores after applying GridSearch:-')\n",
    "for key, classifier in Models.items():\n",
    "    best_classifier = gridsearch(classifier,params[key])\n",
    "    cv_score = cross_val_score(classifier, train_feature, train_label, cv=3)\n",
    "    print('{}: {}'.format(key,round(cv_score.mean()*100.0, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = np.load('Data/defog/test_feature.npy')\n",
    "test_label = np.load('Data/defog/test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "--------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/miniconda3/envs/main/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hanlinn/miniconda3/envs/main/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       0.58      0.44      0.50   2276249\n",
      "           Turn       0.54      0.37      0.44   2273938\n",
      "        Walking       0.00      0.00      0.00   2274579\n",
      "       All_zero       0.78      0.29      0.42   2277255\n",
      "\n",
      "      micro avg       0.61      0.27      0.38   9102021\n",
      "      macro avg       0.48      0.27      0.34   9102021\n",
      "   weighted avg       0.48      0.27      0.34   9102021\n",
      "    samples avg       0.27      0.27      0.27   9102021\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.27320734592899754\n",
      "Accuracy:  0.27320734592899754\n",
      "Avarge Precision:  0.3509588775975744\n"
     ]
    }
   ],
   "source": [
    "base_score,accuracy,av_precision = eval_metrics(best_classifier,\n",
    "                                                test_feature,\n",
    "                                                test_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       1.00      0.93      0.96   2276249\n",
      "           Turn       0.81      0.86      0.83   2273938\n",
      "        Walking       0.89      0.93      0.91   2274579\n",
      "       All_zero       0.80      0.77      0.78   2277255\n",
      "\n",
      "      micro avg       0.87      0.87      0.87   9102021\n",
      "      macro avg       0.87      0.87      0.87   9102021\n",
      "   weighted avg       0.87      0.87      0.87   9102021\n",
      "    samples avg       0.87      0.87      0.87   9102021\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.8708291268499601\n",
      "Accuracy:  0.8708291268499601\n",
      "Avarge Precision:  0.7969772649830308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/miniconda3/envs/main/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for Model_Name, classifier in Models.items(): \n",
    "    print(classifier)\n",
    "    base_score,accuracy,av_precision = eval_metrics(classifier=classifier,\n",
    "                                                    test_features=test_feature,\n",
    "                                                    test_labels= test_label)  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
