{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.737126Z","iopub.status.busy":"2023-05-09T10:43:38.736584Z","iopub.status.idle":"2023-05-09T10:43:38.743491Z","shell.execute_reply":"2023-05-09T10:43:38.741928Z","shell.execute_reply.started":"2023-05-09T10:43:38.737055Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np \n","import matplotlib.pyplot as plt\n","import os\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Import with ID"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.745755Z","iopub.status.busy":"2023-05-09T10:43:38.745258Z","iopub.status.idle":"2023-05-09T10:43:38.765496Z","shell.execute_reply":"2023-05-09T10:43:38.764307Z","shell.execute_reply.started":"2023-05-09T10:43:38.745714Z"},"trusted":true},"outputs":[],"source":["# def preprocess_the_whole_stage(folder_path):\n","#     # create an empty list to store the DataFrames\n","#     dfs = []\n","\n","#     # loop over all files in the folder\n","#     for filename in os.listdir(folder_path):\n","#         if filename.endswith(\".csv\"):\n","#             # extract the ID from the filename (assuming the filename is in the format \"ID.csv\")\n","#             file_id = os.path.splitext(filename)[0]\n","\n","#             # read the CSV file into a DataFrame and add the ID as a new column\n","#             df = pd.read_csv(os.path.join(folder_path, filename))\n","#             df.insert(0, 'ID', file_id)\n","\n","#             # append the DataFrame to the list\n","#             dfs.append(df)\n","\n","#     # concatenate all DataFrames into a single DataFrame\n","#     full_dataset = pd.concat(dfs)\n","    \n","#     # print the resulting DataFrame\n","#     print(full_dataset.head())\n","    \n","#     # Add all zero class\n","#     condition = (full_dataset.StartHesitation == 0) & (full_dataset.Turn == 0) & (full_dataset.Walking == 0)\n","#     condition_2 = (full_dataset.StartHesitation == 1) | (full_dataset.Turn == 1) | (full_dataset.Walking == 1)\n","    \n","#     full_dataset.loc[condition, 'All_zero'] = 1\n","#     full_dataset.loc[condition_2, 'All_zero'] = 0\n","#     print(full_dataset.head())\n","    \n","#     print(\"Cleaning the Dataset\")\n","#     if 'Valid' in full_dataset.columns:\n","#         remove_col = ['ID','Time', 'Valid', 'Task']\n","        \n","#     else:\n","#         remove_col = ['ID','Time']\n","#     print(f\"The remove columns : {remove_col}\")\n","#     clean_dataset = full_dataset.drop(full_dataset[remove_col],axis=1)\n","#     print(clean_dataset.head())\n","    \n","#     # search duplication\n","#     print(f\"Search for Duplication : {clean_dataset.duplicated().sum()}\")\n","#     clean_dataset.drop_duplicates(inplace=True)\n","#     print(f\"Search for Duplication : {clean_dataset.duplicated().sum()}\")\n","#     print(clean_dataset.head())\n","    \n","#     print(\"Checking conditon\")\n","#     condition = (clean_dataset.StartHesitation == 0) & (clean_dataset.Turn == 0 ) & (clean_dataset.Walking == 0)\n","#     total_zero = clean_dataset[condition].shape[0]\n","#     print(f\"Total number where three class are zero: {total_zero}\")\n","#     All_zero = clean_dataset[clean_dataset.All_zero == 1].shape[0]\n","#     print(f\"Total number of All_zero class: {All_zero}\")\n","#     print(f\"Is all zero and Total number zero are equal :{All_zero == total_zero}\")\n","#     a = clean_dataset[clean_dataset.StartHesitation == 1].shape[0]\n","#     print(f\"The number of Class Start Hesitation :{a}\")\n","#     b = clean_dataset[clean_dataset.Walking == 1].shape[0]\n","#     print(f\"The number of Class Walking :{b}\")\n","#     c = clean_dataset[clean_dataset.Turn == 1].shape[0]\n","#     print(f\"The number of Class Turn : {c}\")\n","#     print(f\"Is the toatl number of sample equal to all Four class combine :\"\n","#          f\"{clean_dataset.shape[0] == a + b + c + All_zero}\")\n","    \n","#     return clean_dataset\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.770901Z","iopub.status.busy":"2023-05-09T10:43:38.770314Z","iopub.status.idle":"2023-05-09T10:43:38.784366Z","shell.execute_reply":"2023-05-09T10:43:38.783293Z","shell.execute_reply.started":"2023-05-09T10:43:38.770852Z"},"trusted":true},"outputs":[],"source":["# def oversampling_and_split(clean_dataset):\n","#     feature_col = ['AccV','AccML','AccAP']\n","#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","\n","#     feature_dataset = clean_dataset[feature_col]\n","#     label_dataset = clean_dataset[label_col]\n","#     print(f\"The Feature :{feature_dataset.shape}, \\n\"\n","#           f\"The label {label_dataset.shape}\")\n","    \n","#     print(f\"Because of Four Classes are imbalanced. To get high accuracy, oversampling is used\")\n","#     from imblearn.over_sampling import SMOTE\n","#     import numpy as np\n","#     # Instantiate the MultiLabelUnderSampler\n","#     over_sampler = SMOTE()\n","\n","#     # Undersample the dataset\n","#     X_resampled, y_resampled = over_sampler.fit_resample(feature_dataset.to_numpy(), label_dataset.to_numpy())\n","    \n","#     SMOTE_features_dataset = pd.DataFrame(X_resampled, columns=feature_dataset.columns)\n","    \n","    \n","#     SMOTE_labels_dataset = pd.DataFrame(y_resampled, columns=label_dataset.columns)\n","#     print(f\"The over sampling label shape : {SMOTE_labels_dataset.shape}\")\n","    \n","#     def check_all_four_class_condition(df):\n","#         print(f\"Check all four check condiiton in {df}\")\n","#         a = df[df.StartHesitation == 1].shape[0]\n","#         b = df[df.Turn == 1].shape[0]\n","#         c = df[df.Walking ==1].shape[0]\n","#         d = df[df.All_zero == 1].shape[0]\n","#         print(f\"Number of Start Hesitation : {a}, \\n\"\n","#               f\"Number of Turn : {b}, \\n\"  \n","#               f\"Number of Walking : {c}, \\n\"\n","#               f\"Number of All_zero : {d}\")\n","#         print(\"Is Number of All four class is equal to total sampling :\",\n","#              df.shape[0] == a + b + c + d)\n","        \n","#     check_all_four_class_condition(SMOTE_labels_dataset)\n","    \n","#     oversampling_dataset = pd.concat([SMOTE_features_dataset,SMOTE_labels_dataset], \n","#                                      ignore_index= False, sort=False, axis=1)\n","#     print(f\"The shape of oversampling dataset is : {oversampling_dataset.shape[0]}\")\n","#     print(f\"The number of duplication in dataset : {oversampling_dataset.duplicated().sum()}\")\n","#     # Drop duplication\n","#     oversampling_dataset.drop_duplicates(inplace=True)\n","#     print(f\"The shape of oversampling after remove duplication :{oversampling_dataset.shape}\")\n","    \n","#     # 60% Train Data, 20% Validation Data, 20% Test Data\n","#     # 80% Set Data(60% rain Data, 20% Validation Data) , 20% Test Data\n","    \n","#     from sklearn.model_selection import train_test_split\n","#     import random \n","#     random_seed = 54\n","\n","#     set_data, test_data = train_test_split(oversampling_dataset, test_size=0.2, random_state=True)\n","#     print(f\"The set data shape : {set_data.shape}\\n\"\n","#           f\"The test data shape : {test_data.shape}\\n\"\n","#           f\"Is the dataset still in range : \"\n","#           f\"{oversampling_dataset.shape[0] == set_data.shape[0] + test_data.shape[0]}\")\n","    \n","#     print(f\"Again Search for duplicaiton : \\n \"\n","#           f\"Set Data :{set_data.duplicated().sum()} \\n\"\n","#           f\"Test Data :{test_data.duplicated().sum()}\")\n","    \n","#     check_all_four_class_condition(set_data)\n","#     check_all_four_class_condition(test_data)\n","#     print(\"All task are finish\")\n","    \n","#     return set_data, test_data\n","    "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.926021Z","iopub.status.busy":"2023-05-09T10:43:38.924800Z","iopub.status.idle":"2023-05-09T10:43:38.934262Z","shell.execute_reply":"2023-05-09T10:43:38.932862Z","shell.execute_reply.started":"2023-05-09T10:43:38.925960Z"},"trusted":true},"outputs":[],"source":["# def preprocessing_the_dataset(df):\n","# #     def check_skewness(df):\n","# #     # this can check relation between each column\n","# #         skew_limit=0.75\n","# #         skew_value=df[df.columns].skew()\n","# #         #print(skew_value)\n","# #         skew_col=skew_value[abs(skew_value)>skew_limit]\n","# #         cols=skew_col.index\n","# #         return cols\n","\n","#     import random \n","#     random_seed = 54\n","    \n","#     feature_col = ['AccV','AccML','AccAP']\n","#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","\n","#     # make feature and label\n","#     feature_dataset = df[feature_col]\n","#     label_dataset = df[label_col]\n","    \n","# #     # check skewness and powertransform\n","# #     skew_columns = check_skewness(feature_dataset)\n","# #     print(skew_columns)\n","    \n","# #     print(\"Power Transform start\")\n","# #     from sklearn.preprocessing import PowerTransformer\n","# #     pt=PowerTransformer(standardize=False)  \n","# #     feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n","    \n","# #     print(\"Standardization start\")\n","# #     # Change features data to 0 and 1\n","# #     from sklearn.preprocessing import StandardScaler\n","# #     sc=StandardScaler()\n","# #     feature_dataset=sc.fit_transform(feature_dataset)\n","    \n","#     print(\"Train test split begin\")\n","#     from sklearn.model_selection import train_test_split\n","#     train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n","    \n","#     train_feature = np.array(train_feature) \n","#     valid_feature = np.array(valid_feature)\n","#     train_label  = np.array(train_label)\n","#     valid_label = np.array(valid_label)\n","#     print(\"All task are finish\")\n","    \n","#     return train_feature, valid_feature, train_label, valid_label\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Import Defog Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-05-09T10:43:38.937635Z","iopub.status.busy":"2023-05-09T10:43:38.937051Z","iopub.status.idle":"2023-05-09T10:45:42.854023Z","shell.execute_reply":"2023-05-09T10:45:42.852833Z","shell.execute_reply.started":"2023-05-09T10:43:38.937568Z"},"trusted":true},"outputs":[],"source":["# # specify the folder path\n","# defog_path = \"Data/train/defog\"\n","# clean_defog_dataset = preprocess_the_whole_stage(defog_path)\n","# tdcsfog_path = \"Data/train/tdcsfog\"\n","# clean_tdcsfog_dataset= preprocess_the_whole_stage(tdcsfog_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Mix two dataset and oversplit"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:42.856248Z","iopub.status.busy":"2023-05-09T10:45:42.855803Z","iopub.status.idle":"2023-05-09T10:45:42.880747Z","shell.execute_reply":"2023-05-09T10:45:42.879456Z","shell.execute_reply.started":"2023-05-09T10:45:42.856206Z"},"trusted":true},"outputs":[],"source":["# clean_defog_dataset.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:42.882767Z","iopub.status.busy":"2023-05-09T10:45:42.882323Z","iopub.status.idle":"2023-05-09T10:45:42.901115Z","shell.execute_reply":"2023-05-09T10:45:42.899798Z","shell.execute_reply.started":"2023-05-09T10:45:42.882726Z"},"trusted":true},"outputs":[],"source":["# clean_tdcsfog_dataset.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:42.904310Z","iopub.status.busy":"2023-05-09T10:45:42.903890Z","iopub.status.idle":"2023-05-09T10:45:43.487819Z","shell.execute_reply":"2023-05-09T10:45:43.486504Z","shell.execute_reply.started":"2023-05-09T10:45:42.904278Z"},"trusted":true},"outputs":[],"source":["# clean_dataset = pd.concat([clean_tdcsfog_dataset, clean_defog_dataset ], \n","#                            ignore_index= True, sort=False, axis=0)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:43.489797Z","iopub.status.busy":"2023-05-09T10:45:43.489460Z","iopub.status.idle":"2023-05-09T10:45:43.496557Z","shell.execute_reply":"2023-05-09T10:45:43.495486Z","shell.execute_reply.started":"2023-05-09T10:45:43.489768Z"},"trusted":true},"outputs":[],"source":["# clean_dataset.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:43.498440Z","iopub.status.busy":"2023-05-09T10:45:43.498085Z","iopub.status.idle":"2023-05-09T10:45:43.515613Z","shell.execute_reply":"2023-05-09T10:45:43.514349Z","shell.execute_reply.started":"2023-05-09T10:45:43.498412Z"},"trusted":true},"outputs":[],"source":["# clean_dataset.head()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:43.518084Z","iopub.status.busy":"2023-05-09T10:45:43.517032Z","iopub.status.idle":"2023-05-09T10:54:41.255872Z","shell.execute_reply":"2023-05-09T10:54:41.253714Z","shell.execute_reply.started":"2023-05-09T10:45:43.518030Z"},"trusted":true},"outputs":[],"source":["# set_data,test_data = oversampling_and_split(clean_dataset)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:54:41.258539Z","iopub.status.busy":"2023-05-09T10:54:41.258151Z","iopub.status.idle":"2023-05-09T10:54:56.708707Z","shell.execute_reply":"2023-05-09T10:54:56.707285Z","shell.execute_reply.started":"2023-05-09T10:54:41.258503Z"},"trusted":true},"outputs":[],"source":["# train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_data)\n","# print(f\"{train_feature.shape} , {train_label.shape} , {valid_feature.shape} , {valid_label.shape}, {test_data.shape}\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# def preprocessing_test_dataset(df):\n","\n","#     import random \n","#     random_seed = 54\n","    \n","#     feature_col = ['AccV','AccML','AccAP']\n","#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","\n","#     # make feature and label\n","#     feature_dataset = df[feature_col]\n","#     label_dataset = df[label_col]\n","    \n","#     feature_dataset = np.array(feature_dataset) \n","#     label_dataset  = np.array(label_dataset)\n","#     print(\"All task are finish\")\n","    \n","#     return feature_dataset, label_dataset"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# test_feature, test_label = preprocessing_test_dataset(test_data)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# test_feature.shape, test_label.shape"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# np.save('Data/mix/train_feature.npy',train_feature) \n","# np.save('Data/mix/valid_feature.npy', valid_feature)\n","# np.save('Data/mix/train_label.npy', train_label)\n","# np.save('Data/mix/valid_label.npy',valid_label)\n","# np.save('Data/mix/test_feature.npy',test_feature)\n","# np.save('Data/mix/test_label.npy',test_label)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["train_feature = np.load('Data/mix/train_feature.npy')\n","valid_feature = np.load('Data/mix/valid_feature.npy')\n","test_feature = np.load('Data/mix/test_feature.npy')\n","train_label = np.load('Data/mix/train_label.npy')\n","valid_label = np.load('Data/mix/valid_label.npy')\n","test_label = np.load('Data/mix/test_label.npy')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Build Model"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:54:56.710486Z","iopub.status.busy":"2023-05-09T10:54:56.710154Z","iopub.status.idle":"2023-05-09T10:54:56.719769Z","shell.execute_reply":"2023-05-09T10:54:56.718394Z","shell.execute_reply.started":"2023-05-09T10:54:56.710459Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import classification_report\n","\n","def eval_metrics(classifier, test_features, test_labels):\n","    \n","    # make prediction\n","    predictions   = classifier.predict(test_features)\n","    \n","    base_score   = classifier.score(test_features, test_labels)\n","    accuracy = accuracy_score(test_labels, predictions)\n","    av_precision = average_precision_score(test_labels, predictions)\n","    \n","    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n","    print(\"Classification report\")\n","    print(\"---------------------\",\"\\n\")\n","    print(classification_report(test_labels, predictions, target_names=target_names),\"\\n\")\n","\n","    print(\"Accuracy Measures\")\n","    print(\"---------------------\",\"\\n\")\n","    print(\"Base score: \", base_score)\n","    print(\"Accuracy: \", accuracy)\n","    print(\"Avarge Precision: \", av_precision)\n","    \n","    return base_score,accuracy,av_precision"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:54:56.721656Z","iopub.status.busy":"2023-05-09T10:54:56.721306Z","iopub.status.idle":"2023-05-09T10:54:56.732603Z","shell.execute_reply":"2023-05-09T10:54:56.731199Z","shell.execute_reply.started":"2023-05-09T10:54:56.721628Z"},"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import xgboost as xgb\n","\n","Models = { \n","    # \"Decision Tree\": DecisionTreeClassifier(),      \n","    # \"KNearest\": KNeighborsClassifier(n_jobs=-1),   \n","    \"XGB\" : xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0,max_depth=10, n_estimators=20, learning_rate=0.05), \n","    # \"Extra_T\" : ExtraTreesClassifier(n_estimators=50),\n","    # \"R_forest\" : RandomForestClassifier(n_estimators=50),       \n","}"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tracking URI: 'http://127.0.0.1:5000'\n"]}],"source":["import mlflow\n","import mlflow.tensorflow\n","mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n","print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1. XGB\n","Classification report\n","--------------------- \n","\n"]},{"name":"stderr","output_type":"stream","text":["/home/hanlinn/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","StartHesitation       0.59      0.40      0.48   2598704\n","           Turn       0.64      0.19      0.29   2599871\n","        Walking       0.61      0.16      0.25   2598555\n","       All_zero       0.79      0.52      0.63   2598149\n","\n","      micro avg       0.67      0.32      0.43  10395279\n","      macro avg       0.66      0.32      0.41  10395279\n","   weighted avg       0.66      0.32      0.41  10395279\n","    samples avg       0.32      0.32      0.32  10395279\n"," \n","\n","Accuracy Measures\n","--------------------- \n","\n","Base score:  0.31414933644397613\n","Accuracy:  0.31414933644397613\n","Avarge Precision:  0.38709814269226317\n","Because f1 socre is not quality. The model is skip to saving phase.\n","________________________________________\n"]}],"source":["counter = 1\n","for Model_Name, classifier in Models.items(): \n","    # with mlflow.start_run(nested=True):\n","    print(f\"{counter}. {Model_Name}\")\n","    \n","    with mlflow.start_run():\n","        # fit the model\n","        from joblib import parallel_backend\n","        with parallel_backend('threading', n_jobs=-1):\n","            classifier.fit(train_feature, train_label)\n","        counter = counter + 1\n","        \n","        # Calculate the metrics\n","        base_score,accuracy,av_precision = eval_metrics(classifier,\n","                                                        valid_feature,\n","                                                        valid_label)  \n","        \n","        mlflow.log_param(\"Model\"           , Model_Name)\n","        mlflow.log_param(\"Dataset\" , \"Mix\")\n","        mlflow.log_metric(\"base_score\"     , base_score)\n","        mlflow.log_metric(\"accuracy\"       , accuracy)\n","        mlflow.log_metric(\"av_precision\"   , av_precision)\n","        \n","        if av_precision > 0.95 :\n","            # mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n","            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n","        else :\n","            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n","        \n","        print(\"________________________________________\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-10 16:40:20.031119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","from datetime import datetime"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# epoch = 20\n","# input_shape = 3\n","\n","# inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","# x = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n","# x = layers.LSTM(64)(x) # return vector for whole sequence\n","# x = layers.Dense(16, activation=\"relu\")(x) \n","# outputs = layers.Dense(4, activation=\"softmax\")(x)\n","# model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n","\n","\n","\n","# model.compile(loss=\"categorical_crossentropy\",\n","#               optimizer=tf.keras.optimizers.Adam(),\n","#               metrics=[\"accuracy\"])\n","\n","# model.summary()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Callbacks\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n","                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n","                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n","                                                 patience=3,\n","                                                 verbose=1, # print out when learning rate goes down \n","                                                 min_lr=1e-7)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# train_dataset = tf.data.Dataset.from_tensor_slices((train_feature, train_label))\n","# train_dataset =  train_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n","\n","# valid_dataset = tf.data.Dataset.from_tensor_slices((valid_feature, valid_label))\n","# valid_dataset = valid_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n","\n","# test_dataset = tf.data.Dataset.from_tensor_slices((test_feature,test_label))\n","# test_dataset = test_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n","\n","# train_dataset, valid_dataset, test_dataset"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# # fit the model\n","# import os\n","# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","# tf.get_logger().setLevel('ERROR')\n","# tf.autograph.set_verbosity(1)\n","# tf.set_seed = 42\n","\n","# start = datetime.now()\n","# with mlflow.start_run():\n","#     mlflow.tensorflow.autolog()\n","    \n","#     # fit the model\n","#     # with tf.device('/GPU:0'):\n","#     start = datetime.now()\n","#     history_model = model.fit(train_dataset,\n","#                             batch_size=2048,\n","#                             steps_per_epoch=len(train_dataset),\n","#                             validation_data=valid_dataset,\n","#                             validation_steps=int(len(valid_dataset)),\n","#                             callbacks=[early_stopping, reduce_lr],\n","#                             epochs=epoch) \n","#     end = datetime.now()\n","#     print(f\"The time taken to train the model is {end - start}\")\n","        \n","#     # Evaluate model\n","#     model.evaluate(test_dataset)\n","    \n","#     mlflow.log_param(\"Model\"   , \"LSTM\")\n","#     mlflow.log_param(\"Dataset\" , \"Mix\")\n","#     # mlflow.log_params(model_results)\n","#     mlflow.tensorflow.autolog()\n","    \n","#     # Calculate the metrics\n","#     model_preds_probs   = model.predict(test_feature)\n","#     av_precision    = average_precision_score(test_label, model_preds_probs)\n","#     print(av_precision)\n","#     mlflow.log_metric(\"av_precision\"   , av_precision)\n","\n","# print(\"________________________________________\")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["\n","# # Calculate the metrics\n","# model_preds_probs   = model.predict(test_feature)\n","# av_precision    = average_precision_score(test_label, model_preds_probs)\n","# av_precision"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# # Build the model \n","# input_shape = 3\n","\n","# inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","# x = layers.Dense(32, activation ='relu')(inputs)\n","# x = layers.Dense(64, activation = 'relu')(x)\n","# x = layers.BatchNormalization()(x)\n","# x = layers.Dropout(0.1)(x)\n","# x = layers.Dense(128, activation ='relu')(x)\n","# x = layers.Dense(512, activation = 'relu')(x)\n","# x = layers.BatchNormalization()(x)\n","# x = layers.Dropout(0.25)(x)\n","# x = layers.Dense(1024, activation ='relu')(x)\n","# x = layers.Dense(512, activation = 'relu')(x)\n","# x = layers.BatchNormalization()(x)\n","# x = layers.Dropout(0.5)(x)\n","# x = layers.Dense(128, activation=\"relu\")(x)\n","# x = layers.Dense(64, activation=\"relu\")(x)\n","# x = layers.Dense(32, activation=\"relu\")(x)\n","# outputs = layers.Dense(4, activation=\"softmax\",name=\"output_layer\")(x)      \n","# model_DNN = tf.keras.Model(inputs, outputs) \n","\n","# model_DNN.compile(loss=\"categorical_crossentropy\",\n","#               optimizer=tf.keras.optimizers.Adam(),\n","#               metrics=[\"accuracy\"])\n","\n","# model_DNN.summary()\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# # Fit the model\n","# import os\n","# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","# tf.get_logger().setLevel('ERROR')\n","# tf.autograph.set_verbosity(1)\n","# tf.set_seed = 42\n","# np.random.seed(40)\n","# counter = 1\n","# epoch = 25\n","# Model_Name = 'DNN_model'\n","\n","\n","# with mlflow.start_run():\n","#     mlflow.tensorflow.autolog()\n","    \n","#     # fit the model\n","#     # with tf.device('/GPU:0'):\n","#     start = datetime.now()\n","#     history_model = model_DNN.fit(train_dataset,\n","#                                   batch_size=2048,\n","#                                   steps_per_epoch=len(train_dataset),\n","#                                   validation_data=valid_dataset,\n","#                                   validation_steps=int(len(valid_dataset)),\n","#                                   callbacks=[early_stopping, reduce_lr],\n","#                                   epochs=epoch) \n","#     end = datetime.now()\n","#     print(f\"The time taken to train the model is {end - start}\")\n","        \n","#     # Evaluate model\n","#     model_DNN.evaluate(test_dataset)\n","    \n","#     # Calculate the metrics\n","#     # model_results = eval_metrics(model, \n","#     #                              test_features = test_feature,\n","#     #                              test_labels = test_label)\n","    \n","#     mlflow.log_param(\"Model\"           , Model_Name)\n","#     mlflow.log_param(\"Dataset\" , \"Mix\")\n","#     # mlflow.log_params(model_results)\n","#     mlflow.tensorflow.autolog()\n","    \n","#     # Calculate the metrics\n","#     model_preds_probs   = model_DNN.predict(test_feature)\n","#     av_precision    = average_precision_score(test_label, model_preds_probs)\n","#     print(av_precision)\n","#     mlflow.log_metric(\"av_precision\"   , av_precision)\n","    \n","    \n","#     print(\"________________________________________\")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# model_preds_probs   = model_DNN.predict(test_feature)\n","# av_precision    = average_precision_score(test_label, model_preds_probs)\n","# print(av_precision)\n","# mlflow.log_metric(\"av_precision\"   , av_precision)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Final test"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[],"source":["# import os \n","# import pandas as pd\n","\n","# def import_test_file_from_folder(file_path):\n","#     test_file = pd.read_csv(file_path)\n","#     name = os.path.basename(file_path)\n","#     id_value = name.split('.')[0]\n","#     test_file['Id_value'] = id_value\n","#     test_file['Id'] = test_file['Id_value'].astype(str) + '_' + test_file['Time'].astype(str)\n","#     test_file = test_file[['Id','AccV','AccML','AccAP']]\n","#     return test_file\n","\n","# def preprocessing_test_dataset(df):\n","#     import random \n","#     random_seed = 54\n","    \n","#     feature_col = ['AccV','AccML','AccAP']\n","#     feature_dataset = df[feature_col]\n","\n","#     feature_dataset=np.array(feature_dataset)\n","#     return feature_dataset\n","\n","# def make_prediction(Models, test_submit):\n","#     for Model_Name, classifier in Models.items(): \n","#         test_submit_pred = classifier.predict(test_submit)\n","#     print(f\"The prediciton is {test_submit_pred.shape}\")\n","#     return test_submit_pred\n","\n","# def test_submission_file(test_file, predicit_score_np):\n","#     test_score_df = pd.DataFrame(predicit_score_np, columns=['StartHesitation', 'Turn', 'Walking', 'All_zero'])\n","#     df = pd.concat([test_file, test_score_df], ignore_index= False, sort=False, axis=1)\n","#     select_column = ['Id','StartHesitation', 'Turn', 'Walking']\n","#     submit_dataset = df[select_column]\n","#     return submit_dataset"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["# test_file_path = ('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test')\n","# test_defog_path = testfile_path+'defog/02ab235146.csv'\n","# test_tdcsfog_path = test_file_path+'tdcsfog/003f117e14.csv'"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[],"source":["# test_defog = import_test_file_from_folder(test_defog_path)\n","# test_tdcsfog = import_test_file_from_folder(test_tdcsfog_path)\n","\n","# test_df =  pd.concat([test_tdcsfog, test_defog])\n","# print(test_df.head(), test_df.shape)\n","# test_submit = preprocessing_test_dataset(test_df)\n","# print(test_submit.head())\n","# test_submit_pred = make_prediction(Models, test_submit)\n","# test_submission_file = test_submission_file(test_df, test_submit_pred)\n","# print(test_submission_file.head())"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[],"source":["# test_submission_file.shape"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# submission_sample = pd.read_csv('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv')\n","# submission_sample.shape == test_submission_file.shape"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[],"source":["# test_submission_file.to_csv('submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
