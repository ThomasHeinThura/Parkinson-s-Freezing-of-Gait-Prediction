{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.737126Z","iopub.status.busy":"2023-05-09T10:43:38.736584Z","iopub.status.idle":"2023-05-09T10:43:38.743491Z","shell.execute_reply":"2023-05-09T10:43:38.741928Z","shell.execute_reply.started":"2023-05-09T10:43:38.737055Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np \n","import matplotlib.pyplot as plt\n","import os\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Import with ID"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.745755Z","iopub.status.busy":"2023-05-09T10:43:38.745258Z","iopub.status.idle":"2023-05-09T10:43:38.765496Z","shell.execute_reply":"2023-05-09T10:43:38.764307Z","shell.execute_reply.started":"2023-05-09T10:43:38.745714Z"},"trusted":true},"outputs":[],"source":["# def preprocess_the_whole_stage(folder_path):\n","#     # create an empty list to store the DataFrames\n","#     dfs = []\n","\n","#     # loop over all files in the folder\n","#     for filename in os.listdir(folder_path):\n","#         if filename.endswith(\".csv\"):\n","#             # extract the ID from the filename (assuming the filename is in the format \"ID.csv\")\n","#             file_id = os.path.splitext(filename)[0]\n","\n","#             # read the CSV file into a DataFrame and add the ID as a new column\n","#             df = pd.read_csv(os.path.join(folder_path, filename))\n","#             df.insert(0, 'ID', file_id)\n","\n","#             # append the DataFrame to the list\n","#             dfs.append(df)\n","\n","#     # concatenate all DataFrames into a single DataFrame\n","#     full_dataset = pd.concat(dfs)\n","    \n","#     # print the resulting DataFrame\n","#     print(full_dataset.head())\n","    \n","#     # Add all zero class\n","#     condition = (full_dataset.StartHesitation == 0) & (full_dataset.Turn == 0) & (full_dataset.Walking == 0)\n","#     condition_2 = (full_dataset.StartHesitation == 1) | (full_dataset.Turn == 1) | (full_dataset.Walking == 1)\n","    \n","#     full_dataset.loc[condition, 'All_zero'] = 1\n","#     full_dataset.loc[condition_2, 'All_zero'] = 0\n","#     print(full_dataset.head())\n","    \n","#     print(\"Cleaning the Dataset\")\n","#     if 'Valid' in full_dataset.columns:\n","#         remove_col = ['ID','Time', 'Valid', 'Task']\n","        \n","#     else:\n","#         remove_col = ['ID','Time']\n","#     print(f\"The remove columns : {remove_col}\")\n","#     clean_dataset = full_dataset.drop(full_dataset[remove_col],axis=1)\n","#     print(clean_dataset.head())\n","    \n","#     # search duplication\n","#     print(f\"Search for Duplication : {clean_dataset.duplicated().sum()}\")\n","#     clean_dataset.drop_duplicates(inplace=True)\n","#     print(f\"Search for Duplication : {clean_dataset.duplicated().sum()}\")\n","#     print(clean_dataset.head())\n","    \n","#     print(\"Checking conditon\")\n","#     condition = (clean_dataset.StartHesitation == 0) & (clean_dataset.Turn == 0 ) & (clean_dataset.Walking == 0)\n","#     total_zero = clean_dataset[condition].shape[0]\n","#     print(f\"Total number where three class are zero: {total_zero}\")\n","#     All_zero = clean_dataset[clean_dataset.All_zero == 1].shape[0]\n","#     print(f\"Total number of All_zero class: {All_zero}\")\n","#     print(f\"Is all zero and Total number zero are equal :{All_zero == total_zero}\")\n","#     a = clean_dataset[clean_dataset.StartHesitation == 1].shape[0]\n","#     print(f\"The number of Class Start Hesitation :{a}\")\n","#     b = clean_dataset[clean_dataset.Walking == 1].shape[0]\n","#     print(f\"The number of Class Walking :{b}\")\n","#     c = clean_dataset[clean_dataset.Turn == 1].shape[0]\n","#     print(f\"The number of Class Turn : {c}\")\n","#     print(f\"Is the toatl number of sample equal to all Four class combine :\"\n","#          f\"{clean_dataset.shape[0] == a + b + c + All_zero}\")\n","    \n","#     return clean_dataset\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.770901Z","iopub.status.busy":"2023-05-09T10:43:38.770314Z","iopub.status.idle":"2023-05-09T10:43:38.784366Z","shell.execute_reply":"2023-05-09T10:43:38.783293Z","shell.execute_reply.started":"2023-05-09T10:43:38.770852Z"},"trusted":true},"outputs":[],"source":["# def oversampling_and_split(clean_dataset):\n","#     feature_col = ['AccV','AccML','AccAP']\n","#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","\n","#     feature_dataset = clean_dataset[feature_col]\n","#     label_dataset = clean_dataset[label_col]\n","#     print(f\"The Feature :{feature_dataset.shape}, \\n\"\n","#           f\"The label {label_dataset.shape}\")\n","    \n","#     print(f\"Because of Four Classes are imbalanced. To get high accuracy, oversampling is used\")\n","#     from imblearn.over_sampling import SMOTE\n","#     import numpy as np\n","#     # Instantiate the MultiLabelUnderSampler\n","#     over_sampler = SMOTE()\n","\n","#     # Undersample the dataset\n","#     X_resampled, y_resampled = over_sampler.fit_resample(feature_dataset.to_numpy(), label_dataset.to_numpy())\n","    \n","#     SMOTE_features_dataset = pd.DataFrame(X_resampled, columns=feature_dataset.columns)\n","    \n","    \n","#     SMOTE_labels_dataset = pd.DataFrame(y_resampled, columns=label_dataset.columns)\n","#     print(f\"The over sampling label shape : {SMOTE_labels_dataset.shape}\")\n","    \n","#     def check_all_four_class_condition(df):\n","#         print(f\"Check all four check condiiton in {df}\")\n","#         a = df[df.StartHesitation == 1].shape[0]\n","#         b = df[df.Turn == 1].shape[0]\n","#         c = df[df.Walking ==1].shape[0]\n","#         d = df[df.All_zero == 1].shape[0]\n","#         print(f\"Number of Start Hesitation : {a}, \\n\"\n","#               f\"Number of Turn : {b}, \\n\"  \n","#               f\"Number of Walking : {c}, \\n\"\n","#               f\"Number of All_zero : {d}\")\n","#         print(\"Is Number of All four class is equal to total sampling :\",\n","#              df.shape[0] == a + b + c + d)\n","        \n","#     check_all_four_class_condition(SMOTE_labels_dataset)\n","    \n","#     oversampling_dataset = pd.concat([SMOTE_features_dataset,SMOTE_labels_dataset], \n","#                                      ignore_index= False, sort=False, axis=1)\n","#     print(f\"The shape of oversampling dataset is : {oversampling_dataset.shape[0]}\")\n","#     print(f\"The number of duplication in dataset : {oversampling_dataset.duplicated().sum()}\")\n","#     # Drop duplication\n","#     oversampling_dataset.drop_duplicates(inplace=True)\n","#     print(f\"The shape of oversampling after remove duplication :{oversampling_dataset.shape}\")\n","    \n","#     # 60% Train Data, 20% Validation Data, 20% Test Data\n","#     # 80% Set Data(60% rain Data, 20% Validation Data) , 20% Test Data\n","    \n","#     from sklearn.model_selection import train_test_split\n","#     import random \n","#     random_seed = 54\n","\n","#     set_data, test_data = train_test_split(oversampling_dataset, test_size=0.2, random_state=True)\n","#     print(f\"The set data shape : {set_data.shape}\\n\"\n","#           f\"The test data shape : {test_data.shape}\\n\"\n","#           f\"Is the dataset still in range : \"\n","#           f\"{oversampling_dataset.shape[0] == set_data.shape[0] + test_data.shape[0]}\")\n","    \n","#     print(f\"Again Search for duplicaiton : \\n \"\n","#           f\"Set Data :{set_data.duplicated().sum()} \\n\"\n","#           f\"Test Data :{test_data.duplicated().sum()}\")\n","    \n","#     check_all_four_class_condition(set_data)\n","#     check_all_four_class_condition(test_data)\n","#     print(\"All task are finish\")\n","    \n","#     return set_data, test_data\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:43:38.926021Z","iopub.status.busy":"2023-05-09T10:43:38.924800Z","iopub.status.idle":"2023-05-09T10:43:38.934262Z","shell.execute_reply":"2023-05-09T10:43:38.932862Z","shell.execute_reply.started":"2023-05-09T10:43:38.925960Z"},"trusted":true},"outputs":[],"source":["# def preprocessing_the_dataset(df):\n","# #     def check_skewness(df):\n","# #     # this can check relation between each column\n","# #         skew_limit=0.75\n","# #         skew_value=df[df.columns].skew()\n","# #         #print(skew_value)\n","# #         skew_col=skew_value[abs(skew_value)>skew_limit]\n","# #         cols=skew_col.index\n","# #         return cols\n","\n","#     import random \n","#     random_seed = 54\n","    \n","#     feature_col = ['AccV','AccML','AccAP']\n","#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","\n","#     # make feature and label\n","#     feature_dataset = df[feature_col]\n","#     label_dataset = df[label_col]\n","    \n","# #     # check skewness and powertransform\n","# #     skew_columns = check_skewness(feature_dataset)\n","# #     print(skew_columns)\n","    \n","# #     print(\"Power Transform start\")\n","# #     from sklearn.preprocessing import PowerTransformer\n","# #     pt=PowerTransformer(standardize=False)  \n","# #     feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n","    \n","# #     print(\"Standardization start\")\n","# #     # Change features data to 0 and 1\n","# #     from sklearn.preprocessing import StandardScaler\n","# #     sc=StandardScaler()\n","# #     feature_dataset=sc.fit_transform(feature_dataset)\n","    \n","#     print(\"Train test split begin\")\n","#     from sklearn.model_selection import train_test_split\n","#     train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n","    \n","#     train_feature = np.array(train_feature) \n","#     valid_feature = np.array(valid_feature)\n","#     train_label  = np.array(train_label)\n","#     valid_label = np.array(valid_label)\n","#     print(\"All task are finish\")\n","    \n","#     return train_feature, valid_feature, train_label, valid_label\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Import Defog Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-05-09T10:43:38.937635Z","iopub.status.busy":"2023-05-09T10:43:38.937051Z","iopub.status.idle":"2023-05-09T10:45:42.854023Z","shell.execute_reply":"2023-05-09T10:45:42.852833Z","shell.execute_reply.started":"2023-05-09T10:43:38.937568Z"},"trusted":true},"outputs":[],"source":["# # specify the folder path\n","# defog_path = \"Data/train/defog\"\n","# clean_defog_dataset = preprocess_the_whole_stage(defog_path)\n","# tdcsfog_path = \"Data/train/tdcsfog\"\n","# clean_tdcsfog_dataset= preprocess_the_whole_stage(tdcsfog_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Mix two dataset and oversplit"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:42.856248Z","iopub.status.busy":"2023-05-09T10:45:42.855803Z","iopub.status.idle":"2023-05-09T10:45:42.880747Z","shell.execute_reply":"2023-05-09T10:45:42.879456Z","shell.execute_reply.started":"2023-05-09T10:45:42.856206Z"},"trusted":true},"outputs":[],"source":["# clean_defog_dataset.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:42.882767Z","iopub.status.busy":"2023-05-09T10:45:42.882323Z","iopub.status.idle":"2023-05-09T10:45:42.901115Z","shell.execute_reply":"2023-05-09T10:45:42.899798Z","shell.execute_reply.started":"2023-05-09T10:45:42.882726Z"},"trusted":true},"outputs":[],"source":["# clean_tdcsfog_dataset.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:42.904310Z","iopub.status.busy":"2023-05-09T10:45:42.903890Z","iopub.status.idle":"2023-05-09T10:45:43.487819Z","shell.execute_reply":"2023-05-09T10:45:43.486504Z","shell.execute_reply.started":"2023-05-09T10:45:42.904278Z"},"trusted":true},"outputs":[],"source":["# clean_dataset = pd.concat([clean_tdcsfog_dataset, clean_defog_dataset ], \n","#                            ignore_index= True, sort=False, axis=0)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:43.489797Z","iopub.status.busy":"2023-05-09T10:45:43.489460Z","iopub.status.idle":"2023-05-09T10:45:43.496557Z","shell.execute_reply":"2023-05-09T10:45:43.495486Z","shell.execute_reply.started":"2023-05-09T10:45:43.489768Z"},"trusted":true},"outputs":[],"source":["# clean_dataset.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:43.498440Z","iopub.status.busy":"2023-05-09T10:45:43.498085Z","iopub.status.idle":"2023-05-09T10:45:43.515613Z","shell.execute_reply":"2023-05-09T10:45:43.514349Z","shell.execute_reply.started":"2023-05-09T10:45:43.498412Z"},"trusted":true},"outputs":[],"source":["# clean_dataset.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:45:43.518084Z","iopub.status.busy":"2023-05-09T10:45:43.517032Z","iopub.status.idle":"2023-05-09T10:54:41.255872Z","shell.execute_reply":"2023-05-09T10:54:41.253714Z","shell.execute_reply.started":"2023-05-09T10:45:43.518030Z"},"trusted":true},"outputs":[],"source":["# set_data,test_data = oversampling_and_split(clean_dataset)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:54:41.258539Z","iopub.status.busy":"2023-05-09T10:54:41.258151Z","iopub.status.idle":"2023-05-09T10:54:56.708707Z","shell.execute_reply":"2023-05-09T10:54:56.707285Z","shell.execute_reply.started":"2023-05-09T10:54:41.258503Z"},"trusted":true},"outputs":[],"source":["# train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_data)\n","# print(f\"{train_feature.shape} , {train_label.shape} , {valid_feature.shape} , {valid_label.shape}, {test_data.shape}\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# def preprocessing_test_dataset(df):\n","\n","#     import random \n","#     random_seed = 54\n","    \n","#     feature_col = ['AccV','AccML','AccAP']\n","#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","\n","#     # make feature and label\n","#     feature_dataset = df[feature_col]\n","#     label_dataset = df[label_col]\n","    \n","#     feature_dataset = np.array(feature_dataset) \n","#     label_dataset  = np.array(label_dataset)\n","#     print(\"All task are finish\")\n","    \n","#     return feature_dataset, label_dataset"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# test_feature, test_label = preprocessing_test_dataset(test_data)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# test_feature.shape, test_label.shape"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# np.save('Data/mix/train_feature.npy',train_feature) \n","# np.save('Data/mix/valid_feature.npy', valid_feature)\n","# np.save('Data/mix/train_label.npy', train_label)\n","# np.save('Data/mix/valid_label.npy',valid_label)\n","# np.save('Data/mix/test_feature.npy',test_feature)\n","# np.save('Data/mix/test_label.npy',test_label)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["load_data_path = 'mix'"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["train_feature = np.load(f'Data/{load_data_path}/train_feature.npy')\n","valid_feature = np.load(f'Data/{load_data_path}/valid_feature.npy')\n","test_feature = np.load(f'Data/{load_data_path}/test_feature.npy')\n","train_label = np.load(f'Data/{load_data_path}/train_label.npy')\n","valid_label = np.load(f'Data/{load_data_path}/valid_label.npy')\n","test_label = np.load(f'Data/{load_data_path}/test_label.npy')"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(41581113, 3) , (41581113, 4) , (10395279, 3) , (10395279, 4), (12994098, 3) , (12994098, 4)\n"]}],"source":["print(f\"{train_feature.shape} , {train_label.shape} , {valid_feature.shape} , {valid_label.shape}, {test_feature.shape} , {test_label.shape}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Build Model"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:54:56.710486Z","iopub.status.busy":"2023-05-09T10:54:56.710154Z","iopub.status.idle":"2023-05-09T10:54:56.719769Z","shell.execute_reply":"2023-05-09T10:54:56.718394Z","shell.execute_reply.started":"2023-05-09T10:54:56.710459Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import classification_report\n","\n","def eval_metrics(classifier, test_features, test_labels):\n","    \n","    # make prediction\n","    predictions   = classifier.predict(test_features)\n","    \n","    base_score   = classifier.score(test_features, test_labels)\n","    accuracy = accuracy_score(test_labels, predictions)\n","    av_precision = average_precision_score(test_labels, predictions)\n","    \n","    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n","    print(\"Classification report\")\n","    print(\"---------------------\",\"\\n\")\n","    print(classification_report(test_labels, predictions, target_names=target_names),\"\\n\")\n","\n","    print(\"Accuracy Measures\")\n","    print(\"---------------------\",\"\\n\")\n","    print(\"Base score: \", base_score)\n","    print(\"Accuracy: \", accuracy)\n","    print(\"Avarge Precision: \", av_precision)\n","    \n","    return base_score,accuracy,av_precision"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T10:54:56.721656Z","iopub.status.busy":"2023-05-09T10:54:56.721306Z","iopub.status.idle":"2023-05-09T10:54:56.732603Z","shell.execute_reply":"2023-05-09T10:54:56.731199Z","shell.execute_reply.started":"2023-05-09T10:54:56.721628Z"},"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import xgboost as xgb\n","\n","Models = { \n","    # \"Decision Tree\": DecisionTreeClassifier(),      \n","    \"KNearest\": KNeighborsClassifier(n_jobs=-1),   \n","    \"XGB\" : xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0,\n","                            #   max_depth=10, n_estimators=20, learning_rate=0.05\n","                              ), \n","    # \"Extra_T\" : ExtraTreesClassifier(n_estimators=50),\n","    # \"R_forest\" : RandomForestClassifier(n_estimators=50),       \n","}"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tracking URI: 'http://127.0.0.1:5000'\n"]}],"source":["import mlflow\n","import mlflow.tensorflow\n","mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n","print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1. Decision Tree\n","Classification report\n","--------------------- \n","\n"]},{"name":"stderr","output_type":"stream","text":["/home/hanlinn/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","StartHesitation       0.90      0.92      0.91   2598704\n","           Turn       0.78      0.79      0.79   2599871\n","        Walking       0.87      0.89      0.88   2598555\n","       All_zero       0.80      0.76      0.78   2598149\n","\n","      micro avg       0.84      0.84      0.84  10395279\n","      macro avg       0.84      0.84      0.84  10395279\n","   weighted avg       0.84      0.84      0.84  10395279\n","    samples avg       0.84      0.84      0.84  10395279\n"," \n","\n","Accuracy Measures\n","--------------------- \n","\n","Base score:  0.8397757289631187\n","Accuracy:  0.8397757289631187\n","Avarge Precision:  0.7479029046152332\n","Because f1 socre is not quality. The model is skip to saving phase.\n","________________________________________\n","2. KNearest\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m counter \u001b[39m=\u001b[39m counter \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Calculate the metrics\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m base_score,accuracy,av_precision \u001b[39m=\u001b[39m eval_metrics(classifier,\n\u001b[1;32m     15\u001b[0m                                                 valid_feature,\n\u001b[1;32m     16\u001b[0m                                                 valid_label)  \n\u001b[1;32m     18\u001b[0m mlflow\u001b[39m.\u001b[39mlog_param(\u001b[39m\"\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m\"\u001b[39m           , Model_Name)\n\u001b[1;32m     19\u001b[0m mlflow\u001b[39m.\u001b[39mlog_param(\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m , \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mload_data_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n","Cell \u001b[0;32mIn[20], line 11\u001b[0m, in \u001b[0;36meval_metrics\u001b[0;34m(classifier, test_features, test_labels)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_metrics\u001b[39m(classifier, test_features, test_labels):\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     \u001b[39m# make prediction\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     predictions   \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(test_features)\n\u001b[0;32m---> 11\u001b[0m     base_score   \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mscore(test_features, test_labels)\n\u001b[1;32m     12\u001b[0m     accuracy \u001b[39m=\u001b[39m accuracy_score(test_labels, predictions)\n\u001b[1;32m     13\u001b[0m     av_precision \u001b[39m=\u001b[39m average_precision_score(test_labels, predictions)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:668\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 668\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:252\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m k, classes_k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes_):\n\u001b[1;32m    251\u001b[0m     \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m         mode, _ \u001b[39m=\u001b[39m _mode(_y[neigh_ind, k], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    253\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         mode, _ \u001b[39m=\u001b[39m weighted_mode(_y[neigh_ind, k], weights, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/fixes.py:169\u001b[0m, in \u001b[0;36m_mode\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mode\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m sp_version \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m parse_version(\u001b[39m\"\u001b[39m\u001b[39m1.9.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 169\u001b[0m         mode \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49mstats\u001b[39m.\u001b[39;49mmode(a, axis\u001b[39m=\u001b[39;49maxis, keepdims\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m sp_version \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m parse_version(\u001b[39m\"\u001b[39m\u001b[39m1.10.999\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    171\u001b[0m             \u001b[39m# scipy.stats.mode has changed returned array shape with axis=None\u001b[39;00m\n\u001b[1;32m    172\u001b[0m             \u001b[39m# and keepdims=True, see https://github.com/scipy/scipy/pull/17561\u001b[39;00m\n\u001b[1;32m    173\u001b[0m             \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m/usr/lib/python3.11/site-packages/scipy/stats/_stats_py.py:633\u001b[0m, in \u001b[0;36mmode\u001b[0;34m(a, axis, nan_policy, keepdims)\u001b[0m\n\u001b[1;32m    631\u001b[0m counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(a_view\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint_)\n\u001b[1;32m    632\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m inds:\n\u001b[0;32m--> 633\u001b[0m     modes[ind], counts[ind] \u001b[39m=\u001b[39m _mode1D(a_view[ind])\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m keepdims:\n\u001b[1;32m    636\u001b[0m     newshape \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(a\u001b[39m.\u001b[39mshape)\n","File \u001b[0;32m/usr/lib/python3.11/site-packages/scipy/stats/_stats_py.py:621\u001b[0m, in \u001b[0;36mmode.<locals>._mode1D\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mode1D\u001b[39m(a):\n\u001b[1;32m    620\u001b[0m     vals, cnts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(a, return_counts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 621\u001b[0m     \u001b[39mreturn\u001b[39;00m vals[cnts\u001b[39m.\u001b[39margmax()], cnts\u001b[39m.\u001b[39;49mmax()\n","File \u001b[0;32m/usr/lib/python3.11/site-packages/numpy/core/_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_amax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[39mNone\u001b[39;00m, out, keepdims, initial, where)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["counter = 1\n","for Model_Name, classifier in Models.items(): \n","    # with mlflow.start_run(nested=True):\n","    print(f\"{counter}. {Model_Name}\")\n","    \n","    with mlflow.start_run():\n","        # fit the model\n","        from joblib import parallel_backend\n","        with parallel_backend('threading', n_jobs=-1):\n","            classifier.fit(train_feature, train_label)\n","        counter = counter + 1\n","        \n","        # Calculate the metrics\n","        base_score,accuracy,av_precision = eval_metrics(classifier,\n","                                                        valid_feature,\n","                                                        valid_label)  \n","        \n","        mlflow.log_param(\"Model\"           , Model_Name)\n","        mlflow.log_param(\"Dataset\" , f'{load_data_path}')\n","        mlflow.log_metric(\"base_score\"     , base_score)\n","        mlflow.log_metric(\"accuracy\"       , accuracy)\n","        mlflow.log_metric(\"av_precision\"   , av_precision)\n","        \n","        if av_precision > 0.95 :\n","            # mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n","            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n","        else :\n","            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n","        \n","        print(\"________________________________________\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epoch = 5\n","input_shape = 4\n","\n","inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","x = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n","x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n","x = layers.Bidirectional(layers.LSTM(64))(x)\n","x = layers.Dense(32, activation=\"relu\")(x) \n","x = layers.Dense(16, activation=\"relu\")(x) \n","outputs = layers.Dense(4, activation=\"softmax\")(x)\n","model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n","\n","\n","\n","model.compile(loss=\"categorical_crossentropy\",\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=[\"accuracy\"])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Callbacks\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n","                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n","                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n","                                                 patience=3,\n","                                                 verbose=1, # print out when learning rate goes down \n","                                                 min_lr=1e-7)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_tensor_slices((train_feature, train_label))\n","train_dataset =  train_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n","\n","valid_dataset = tf.data.Dataset.from_tensor_slices((valid_feature, valid_label))\n","valid_dataset = valid_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_feature,test_label))\n","test_dataset = test_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n","\n","train_dataset, valid_dataset, test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# fit the model\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","tf.get_logger().setLevel('ERROR')\n","tf.autograph.set_verbosity(1)\n","tf.set_seed = 42\n","\n","start = datetime.now()\n","with mlflow.start_run():\n","    mlflow.tensorflow.autolog()\n","    \n","    # fit the model\n","    # with tf.device('/GPU:0'):\n","    start = datetime.now()\n","    history_model = model.fit(train_dataset,\n","                            batch_size=2048,\n","                            steps_per_epoch=len(train_dataset),\n","                            validation_data=valid_dataset,\n","                            validation_steps=int(len(valid_dataset)),\n","                            callbacks=[early_stopping, reduce_lr],\n","                            epochs=epoch) \n","    end = datetime.now()\n","    print(f\"The time taken to train the model is {end - start}\")\n","        \n","    # Evaluate model\n","    model.evaluate(test_dataset)\n","    \n","    mlflow.log_param(\"Model\"   , \"LSTM\")\n","    mlflow.log_param(\"Dataset\" , f'{load_data_path}')\n","    # mlflow.log_params(model_results)\n","    mlflow.tensorflow.autolog()\n","    \n","    # Calculate the metrics\n","    model_preds_probs   = model.predict(test_feature)\n","    av_precision    = average_precision_score(test_label, model_preds_probs)\n","    print(av_precision)\n","    mlflow.log_metric(\"av_precision\"   , av_precision)\n","\n","print(\"________________________________________\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# # Calculate the metrics\n","# model_preds_probs   = model.predict(test_feature)\n","# av_precision    = average_precision_score(test_label, model_preds_probs)\n","# av_precision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build the model \n","input_shape = 4\n","\n","inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","x = layers.Dense(32, activation ='relu')(inputs)\n","x = layers.Dense(64, activation = 'relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(128, activation ='relu')(x)\n","x = layers.Dense(512, activation = 'relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.25)(x)\n","x = layers.Dense(1024, activation ='relu')(x)\n","x = layers.Dense(512, activation = 'relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.5)(x)\n","x = layers.Dense(128, activation=\"relu\")(x)\n","x = layers.Dense(64, activation=\"relu\")(x)\n","x = layers.Dense(32, activation=\"relu\")(x)\n","outputs = layers.Dense(4, activation=\"softmax\",name=\"output_layer\")(x)      \n","model_DNN = tf.keras.Model(inputs, outputs) \n","\n","model_DNN.compile(loss=\"categorical_crossentropy\",\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=[\"accuracy\"])\n","\n","model_DNN.summary()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the model\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","tf.get_logger().setLevel('ERROR')\n","tf.autograph.set_verbosity(1)\n","tf.set_seed = 42\n","np.random.seed(40)\n","counter = 1\n","epoch = 5\n","Model_Name = 'DNN_model'\n","\n","\n","with mlflow.start_run():\n","    mlflow.tensorflow.autolog()\n","    \n","    # fit the model\n","    # with tf.device('/GPU:0'):\n","    start = datetime.now()\n","    history_model = model_DNN.fit(train_dataset,\n","                                  batch_size=2048,\n","                                  steps_per_epoch=len(train_dataset),\n","                                  validation_data=valid_dataset,\n","                                  validation_steps=int(len(valid_dataset)),\n","                                  callbacks=[early_stopping, reduce_lr],\n","                                  epochs=epoch) \n","    end = datetime.now()\n","    print(f\"The time taken to train the model is {end - start}\")\n","        \n","    # Evaluate model\n","    model_DNN.evaluate(test_dataset)\n","    \n","    # Calculate the metrics\n","    # model_results = eval_metrics(model, \n","    #                              test_features = test_feature,\n","    #                              test_labels = test_label)\n","    \n","    mlflow.log_param(\"Model\"           , Model_Name)\n","    mlflow.log_param(\"Dataset\" , f'{load_data_path}')\n","    # mlflow.log_params(model_results)\n","    mlflow.tensorflow.autolog()\n","    \n","    # Calculate the metrics\n","    model_preds_probs   = model_DNN.predict(test_feature)\n","    av_precision    = average_precision_score(test_label, model_preds_probs)\n","    print(av_precision)\n","    mlflow.log_metric(\"av_precision\"   , av_precision)\n","    \n","    \n","    print(\"________________________________________\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model_preds_probs   = model_DNN.predict(test_feature)\n","# av_precision    = average_precision_score(test_label, model_preds_probs)\n","# print(av_precision)\n","# mlflow.log_metric(\"av_precision\"   , av_precision)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Final test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import os \n","# import pandas as pd\n","\n","# def import_test_file_from_folder(file_path):\n","#     test_file = pd.read_csv(file_path)\n","#     name = os.path.basename(file_path)\n","#     id_value = name.split('.')[0]\n","#     test_file['Id_value'] = id_value\n","#     test_file['Id'] = test_file['Id_value'].astype(str) + '_' + test_file['Time'].astype(str)\n","#     test_file = test_file[['Id','AccV','AccML','AccAP']]\n","#     return test_file\n","\n","# def preprocessing_test_dataset(df):\n","#     import random \n","#     random_seed = 54\n","    \n","#     feature_col = ['AccV','AccML','AccAP']\n","#     feature_dataset = df[feature_col]\n","\n","#     feature_dataset=np.array(feature_dataset)\n","#     return feature_dataset\n","\n","# def make_prediction(Models, test_submit):\n","#     for Model_Name, classifier in Models.items(): \n","#         test_submit_pred = classifier.predict(test_submit)\n","#     print(f\"The prediciton is {test_submit_pred.shape}\")\n","#     return test_submit_pred\n","\n","# def test_submission_file(test_file, predicit_score_np):\n","#     test_score_df = pd.DataFrame(predicit_score_np, columns=['StartHesitation', 'Turn', 'Walking', 'All_zero'])\n","#     df = pd.concat([test_file, test_score_df], ignore_index= False, sort=False, axis=1)\n","#     select_column = ['Id','StartHesitation', 'Turn', 'Walking']\n","#     submit_dataset = df[select_column]\n","#     return submit_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_file_path = ('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test')\n","# test_defog_path = testfile_path+'defog/02ab235146.csv'\n","# test_tdcsfog_path = test_file_path+'tdcsfog/003f117e14.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_defog = import_test_file_from_folder(test_defog_path)\n","# test_tdcsfog = import_test_file_from_folder(test_tdcsfog_path)\n","\n","# test_df =  pd.concat([test_tdcsfog, test_defog])\n","# print(test_df.head(), test_df.shape)\n","# test_submit = preprocessing_test_dataset(test_df)\n","# print(test_submit.head())\n","# test_submit_pred = make_prediction(Models, test_submit)\n","# test_submission_file = test_submission_file(test_df, test_submit_pred)\n","# print(test_submission_file.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_submission_file.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# submission_sample = pd.read_csv('/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv')\n","# submission_sample.shape == test_submission_file.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_submission_file.to_csv('submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
