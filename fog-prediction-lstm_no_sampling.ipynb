{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### This time start without sampling and test with LSTM\n","\n","Without sampling usually failed bz it is unbalanced class dataset and you need to oversampling and undersampling.\n","\n","Decision Tree ( av-precison = 0.258728) without sampling  \n","DNN-LSTM ( av-precison = 0.25872) without sampling"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Test without oversmapling"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:25:53.109746Z","iopub.status.busy":"2023-05-11T02:25:53.109378Z","iopub.status.idle":"2023-05-11T02:25:53.123526Z","shell.execute_reply":"2023-05-11T02:25:53.121913Z","shell.execute_reply.started":"2023-05-11T02:25:53.109712Z"},"trusted":true},"outputs":[],"source":["import os \n","import numpy as np \n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-11T02:25:53.611254Z","iopub.status.busy":"2023-05-11T02:25:53.608492Z","iopub.status.idle":"2023-05-11T02:25:53.630016Z","shell.execute_reply":"2023-05-11T02:25:53.629035Z","shell.execute_reply.started":"2023-05-11T02:25:53.611214Z"},"trusted":true},"outputs":[],"source":["def preprocess_the_whole_stage(folder_path):\n","    # create an empty list to store the DataFrames\n","    dfs = []\n","\n","    # loop over all files in the folder\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".csv\"):\n","            # extract the ID from the filename (assuming the filename is in the format \"ID.csv\")\n","            file_id = os.path.splitext(filename)[0]\n","\n","            # read the CSV file into a DataFrame and add the ID as a new column\n","            df = pd.read_csv(os.path.join(folder_path, filename))\n","            df.insert(0, 'ID', file_id)\n","\n","            # append the DataFrame to the list\n","            dfs.append(df)\n","\n","    # concatenate all DataFrames into a single DataFrame\n","    full_dataset = pd.concat(dfs)\n","    \n","    # print the resulting DataFrame\n","    print(full_dataset.head())\n","    \n","    # Add all zero class\n","    condition = (full_dataset.StartHesitation == 0) & (full_dataset.Turn == 0) & (full_dataset.Walking == 0)\n","    condition_2 = (full_dataset.StartHesitation == 1) | (full_dataset.Turn == 1) | (full_dataset.Walking == 1)\n","    \n","    full_dataset.loc[condition, 'All_zero'] = 1\n","    full_dataset.loc[condition_2, 'All_zero'] = 0\n","    print(full_dataset.head())\n","    \n","    print(\"Cleaning the Dataset\")\n","    if 'Valid' in full_dataset.columns:\n","        remove_col = ['ID', 'Valid', 'Task']\n","        \n","    else:\n","        remove_col = ['ID']\n","    print(f\"The remove columns : {remove_col}\")\n","    clean_dataset = full_dataset.drop(full_dataset[remove_col],axis=1)\n","    print(clean_dataset.head())\n","    \n","    # search duplication\n","    print(f\"Search for Duplication : {clean_dataset.duplicated().sum()}\")\n","    clean_dataset.drop_duplicates(inplace=True)\n","    print(f\"Search for Duplication : {clean_dataset.duplicated().sum()}\")\n","    print(clean_dataset.head())\n","    \n","    print(\"Checking conditon\")\n","    condition = (clean_dataset.StartHesitation == 0) & (clean_dataset.Turn == 0 ) & (clean_dataset.Walking == 0)\n","    total_zero = clean_dataset[condition].shape[0]\n","    print(f\"Total number where three class are zero: {total_zero}\")\n","    All_zero = clean_dataset[clean_dataset.All_zero == 1].shape[0]\n","    print(f\"Total number of All_zero class: {All_zero}\")\n","    print(f\"Is all zero and Total number zero are equal :{All_zero == total_zero}\")\n","    a = clean_dataset[clean_dataset.StartHesitation == 1].shape[0]\n","    print(f\"The number of Class Start Hesitation :{a}\")\n","    b = clean_dataset[clean_dataset.Walking == 1].shape[0]\n","    print(f\"The number of Class Walking :{b}\")\n","    c = clean_dataset[clean_dataset.Turn == 1].shape[0]\n","    print(f\"The number of Class Turn : {c}\")\n","    print(f\"Is the toatl number of sample equal to all Four class combine :\"\n","         f\"{clean_dataset.shape[0] == a + b + c + All_zero}\")\n","    \n","    return clean_dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:25:54.350135Z","iopub.status.busy":"2023-05-11T02:25:54.348771Z","iopub.status.idle":"2023-05-11T02:25:54.361915Z","shell.execute_reply":"2023-05-11T02:25:54.360764Z","shell.execute_reply.started":"2023-05-11T02:25:54.350096Z"},"trusted":true},"outputs":[],"source":["def oversampling_and_split(clean_dataset):\n","    feature_col = ['Time', 'AccV','AccML','AccAP']\n","    label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","    \n","    feature_dataset = clean_dataset[feature_col]\n","    label_dataset = clean_dataset[label_col]\n","    print(f\"The Feature :{feature_dataset.shape}, \\n\"\n","          f\"The label {label_dataset.shape}\")\n","    \n","    \n","    def check_all_four_class_condition(df):\n","        print(f\"Check all four check condiiton in {df}\")\n","        a = df[df.StartHesitation == 1].shape[0]\n","        b = df[df.Turn == 1].shape[0]\n","        c = df[df.Walking ==1].shape[0]\n","        d = df[df.All_zero == 1].shape[0]\n","        print(f\"Number of Start Hesitation : {a}, \\n\"\n","              f\"Number of Turn : {b}, \\n\"  \n","              f\"Number of Walking : {c}, \\n\"\n","              f\"Number of All_zero : {d}\")\n","        print(\"Is Number of All four class is equal to total sampling :\",\n","             df.shape[0] == a + b + c + d)\n","\n","    \n","    oversampling_dataset = pd.concat([feature_dataset,label_dataset], \n","                                     ignore_index= False, sort=False, axis=1)\n","    \n","    print(f\"The shape of oversampling dataset is : {oversampling_dataset.shape[0]}\")\n","    print(f\"The number of duplication in dataset : {oversampling_dataset.duplicated().sum()}\")\n","    # Drop duplication\n","    oversampling_dataset.drop_duplicates(inplace=True)\n","    print(f\"The shape of oversampling after remove duplication :{oversampling_dataset.shape}\")\n","    \n","    # 60% Train Data, 20% Validation Data, 20% Test Data\n","    # 80% Set Data(60% rain Data, 20% Validation Data) , 20% Test Data\n","    \n","    from sklearn.model_selection import train_test_split\n","    import random \n","    random_seed = 54\n","\n","    set_data, test_data = train_test_split(oversampling_dataset, test_size=0.2, random_state=True)\n","    print(f\"The set data shape : {set_data.shape}\\n\"\n","          f\"The test data shape : {test_data.shape}\\n\"\n","          f\"Is the dataset still in range : \"\n","          f\"{oversampling_dataset.shape[0] == set_data.shape[0] + test_data.shape[0]}\")\n","    \n","    print(f\"Again Search for duplicaiton : \\n \"\n","          f\"Set Data :{set_data.duplicated().sum()} \\n\"\n","          f\"Test Data :{test_data.duplicated().sum()}\")\n","    \n","    check_all_four_class_condition(set_data)\n","    check_all_four_class_condition(test_data)\n","    print(\"All task are finish\")\n","    \n","    return set_data, test_data\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:25:54.830971Z","iopub.status.busy":"2023-05-11T02:25:54.830601Z","iopub.status.idle":"2023-05-11T02:25:54.839488Z","shell.execute_reply":"2023-05-11T02:25:54.838385Z","shell.execute_reply.started":"2023-05-11T02:25:54.830942Z"},"trusted":true},"outputs":[],"source":["def preprocessing_the_dataset(df):\n","    def check_skewness(df):\n","    # this can check relation between each column\n","        skew_limit=0.75\n","        skew_value=df[df.columns].skew()\n","        #print(skew_value)\n","        skew_col=skew_value[abs(skew_value)>skew_limit]\n","        cols=skew_col.index\n","        return cols\n","\n","    import random \n","    random_seed = 54\n","    \n","    feature_col = ['Time','AccV','AccML','AccAP']\n","    label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n","\n","    # make feature and label\n","    feature_dataset = df[feature_col]\n","    label_dataset = df[label_col]\n","    \n","    # check skewness and powertransform\n","    skew_columns = check_skewness(feature_dataset)\n","    print(skew_columns)\n","    \n","    print(\"Power Transform start\")\n","    from sklearn.preprocessing import PowerTransformer\n","    pt=PowerTransformer(standardize=False)  \n","    feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n","    \n","    print(\"Standardization start\")\n","    # Change features data to 0 and 1\n","    from sklearn.preprocessing import StandardScaler\n","    sc=StandardScaler()\n","    feature_dataset=sc.fit_transform(feature_dataset)\n","    \n","    print(\"Train test split begin\")\n","    from sklearn.model_selection import train_test_split\n","    train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n","    \n","    train_feature = np.array(train_feature) \n","    valid_feature = np.array(valid_feature)\n","    train_label  = np.array(train_label)\n","    valid_label = np.array(valid_label)\n","    print(\"All task are finish\")\n","    \n","    return train_feature, valid_feature, train_label, valid_label\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:25:55.340690Z","iopub.status.busy":"2023-05-11T02:25:55.340346Z","iopub.status.idle":"2023-05-11T02:28:00.142000Z","shell.execute_reply":"2023-05-11T02:28:00.140938Z","shell.execute_reply.started":"2023-05-11T02:25:55.340662Z"},"trusted":true},"outputs":[],"source":["# specify the folder path\n","defog_path = \"Data/train/defog\"\n","clean_defog_dataset = preprocess_the_whole_stage(defog_path)\n","tdcsfog_path = \"Data/train/tdcsfog\"\n","clean_tdcsfog_dataset= preprocess_the_whole_stage(tdcsfog_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:28:00.144540Z","iopub.status.busy":"2023-05-11T02:28:00.144173Z","iopub.status.idle":"2023-05-11T02:28:00.161284Z","shell.execute_reply":"2023-05-11T02:28:00.159996Z","shell.execute_reply.started":"2023-05-11T02:28:00.144506Z"},"trusted":true},"outputs":[],"source":["clean_defog_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:28:00.163242Z","iopub.status.busy":"2023-05-11T02:28:00.162796Z","iopub.status.idle":"2023-05-11T02:28:00.179969Z","shell.execute_reply":"2023-05-11T02:28:00.179156Z","shell.execute_reply.started":"2023-05-11T02:28:00.163208Z"},"trusted":true},"outputs":[],"source":["clean_tdcsfog_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:28:00.182614Z","iopub.status.busy":"2023-05-11T02:28:00.182255Z","iopub.status.idle":"2023-05-11T02:28:01.016744Z","shell.execute_reply":"2023-05-11T02:28:01.015806Z","shell.execute_reply.started":"2023-05-11T02:28:00.182583Z"},"trusted":true},"outputs":[],"source":["clean_dataset = pd.concat([clean_tdcsfog_dataset, clean_defog_dataset ], \n","                           ignore_index= True, sort=False, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:28:01.018706Z","iopub.status.busy":"2023-05-11T02:28:01.018331Z","iopub.status.idle":"2023-05-11T02:29:39.531277Z","shell.execute_reply":"2023-05-11T02:29:39.530244Z","shell.execute_reply.started":"2023-05-11T02:28:01.018670Z"},"trusted":true},"outputs":[],"source":["set_data,test_data = oversampling_and_split(clean_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:29:39.533163Z","iopub.status.busy":"2023-05-11T02:29:39.532547Z","iopub.status.idle":"2023-05-11T02:29:44.543935Z","shell.execute_reply":"2023-05-11T02:29:44.543010Z","shell.execute_reply.started":"2023-05-11T02:29:39.533133Z"},"trusted":true},"outputs":[],"source":["train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_data)\n","print(f\"{train_feature.shape} , {train_label.shape} , {valid_feature.shape} , {valid_label.shape}, {test_data.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:29:44.546493Z","iopub.status.busy":"2023-05-11T02:29:44.546147Z","iopub.status.idle":"2023-05-11T02:29:44.552104Z","shell.execute_reply":"2023-05-11T02:29:44.551171Z","shell.execute_reply.started":"2023-05-11T02:29:44.546461Z"},"trusted":true},"outputs":[],"source":["def preprocessing_test_dataset(df):\n","\n","    import random \n","    random_seed = 54\n","    \n","    feature_col = ['Time','AccV','AccML','AccAP']\n","    label_col = ['StartHesitation','Turn','Walking']\n","\n","    # make feature and label\n","    feature_dataset = df[feature_col]\n","    label_dataset = df[label_col]\n","    \n","    def check_skewness(df):\n","    # this can check relation between each column\n","        skew_limit=0.75\n","        skew_value=df[df.columns].skew()\n","        #print(skew_value)\n","        skew_col=skew_value[abs(skew_value)>skew_limit]\n","        cols=skew_col.index\n","        return cols\n","    \n","    # check skewness and powertransform\n","    skew_columns = check_skewness(feature_dataset)\n","    print(skew_columns)\n","    \n","    print(\"Power Transform start\")\n","    from sklearn.preprocessing import PowerTransformer\n","    pt=PowerTransformer(standardize=False)  \n","    feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n","    \n","    print(\"Standardization start\")\n","    # Change features data to 0 and 1\n","    from sklearn.preprocessing import StandardScaler\n","    sc=StandardScaler()\n","    feature_dataset=sc.fit_transform(feature_dataset)\n","\n","    \n","\n","    feature_dataset = np.array(feature_dataset) \n","    label_dataset  = np.array(label_dataset)\n","    print(\"All task are finish\")\n","    \n","    return feature_dataset, label_dataset\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:29:53.315637Z","iopub.status.busy":"2023-05-11T02:29:53.315268Z","iopub.status.idle":"2023-05-11T02:29:53.574900Z","shell.execute_reply":"2023-05-11T02:29:53.573905Z","shell.execute_reply.started":"2023-05-11T02:29:53.315607Z"},"trusted":true},"outputs":[],"source":["test_feature, test_label = preprocessing_test_dataset(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["load_data_path = 'Time_no_sampling'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.save(f'Data/{load_data_path}/train_feature.npy',train_feature) \n","np.save(f'Data/{load_data_path}/valid_feature.npy', valid_feature)\n","np.save(f'Data/{load_data_path}/train_label.npy', train_label)\n","np.save(f'Data/{load_data_path}/valid_label.npy',valid_label)\n","np.save(f'Data/{load_data_path}/test_feature.npy',test_feature)\n","np.save(f'Data/{load_data_path}/test_label.npy',test_label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train_feature = np.load(f'Data/{load_data_path}/train_feature.npy')\n","# valid_feature = np.load(f'Data/{load_data_path}/valid_feature.npy')\n","# test_feature = np.load(f'Data/{load_data_path}/test_feature.npy')\n","# train_label = np.load(f'Data/{load_data_path}/train_label.npy')\n","# valid_label = np.load(f'Data/{load_data_path}/valid_label.npy')\n","# test_label = np.load(f'Data/{load_data_path}/test_label.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:29:54.506296Z","iopub.status.busy":"2023-05-11T02:29:54.505937Z","iopub.status.idle":"2023-05-11T02:29:54.511009Z","shell.execute_reply":"2023-05-11T02:29:54.510079Z","shell.execute_reply.started":"2023-05-11T02:29:54.506268Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import classification_report\n","\n","def eval_metrics(classifier, test_features, test_labels):\n","    \n","    # make prediction\n","    predictions   = classifier.predict(test_features)\n","    \n","    base_score   = classifier.score(test_features, test_labels)\n","    accuracy = accuracy_score(test_labels, predictions)\n","    av_precision = average_precision_score(test_labels, predictions)\n","    \n","    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n","    print(\"Classification report\")\n","    print(\"---------------------\",\"\\n\")\n","    print(classification_report(test_labels, predictions, target_names=target_names),\"\\n\")\n","\n","    print(\"Accuracy Measures\")\n","    print(\"---------------------\",\"\\n\")\n","    print(\"Base score: \", base_score)\n","    print(\"Accuracy: \", accuracy)\n","    print(\"Avarge Precision: \", av_precision)\n","    \n","    return base_score,accuracy,av_precision"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:29:54.989304Z","iopub.status.busy":"2023-05-11T02:29:54.988945Z","iopub.status.idle":"2023-05-11T02:29:54.993848Z","shell.execute_reply":"2023-05-11T02:29:54.992667Z","shell.execute_reply.started":"2023-05-11T02:29:54.989275Z"},"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import xgboost as xgb\n","\n","Models = { \n","    \"Decision Tree\": DecisionTreeClassifier(),      \n","    \"KNearest\": KNeighborsClassifier(n_jobs=-1),   \n","    \"XGB\" : xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0,\n","                            #   max_depth=10, n_estimators=20, learning_rate=0.05\n","                              ), \n","    # \"Extra_T\" : ExtraTreesClassifier(n_estimators=50),\n","    # \"R_forest\" : RandomForestClassifier(n_estimators=50),       \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import mlflow\n","import mlflow.tensorflow\n","mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n","print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:29:55.332692Z","iopub.status.busy":"2023-05-11T02:29:55.331993Z","iopub.status.idle":"2023-05-11T02:29:55.338373Z","shell.execute_reply":"2023-05-11T02:29:55.337350Z","shell.execute_reply.started":"2023-05-11T02:29:55.332652Z"},"trusted":true},"outputs":[],"source":["counter = 1\n","for Model_Name, classifier in Models.items(): \n","    # with mlflow.start_run(nested=True):\n","    print(f\"{counter}. {Model_Name}\")\n","    \n","    with mlflow.start_run():\n","        # fit the model\n","        from joblib import parallel_backend\n","        with parallel_backend('threading', n_jobs=-1):\n","            classifier.fit(train_feature, train_label)\n","        counter = counter + 1\n","        \n","        # Calculate the metrics\n","        base_score,accuracy,av_precision = eval_metrics(classifier,\n","                                                        valid_feature,\n","                                                        valid_label)  \n","        \n","        mlflow.log_param(\"Model\"           , Model_Name)\n","        mlflow.log_param(\"Dataset\" , f'{load_data_path}')\n","        mlflow.log_metric(\"base_score\"     , base_score)\n","        mlflow.log_metric(\"accuracy\"       , accuracy)\n","        mlflow.log_metric(\"av_precision\"   , av_precision)\n","        \n","        if av_precision > 0.95 :\n","            # mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n","            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n","        else :\n","            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n","        \n","        print(\"________________________________________\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:29:56.874561Z","iopub.status.busy":"2023-05-11T02:29:56.873966Z","iopub.status.idle":"2023-05-11T02:30:03.252072Z","shell.execute_reply":"2023-05-11T02:30:03.251043Z","shell.execute_reply.started":"2023-05-11T02:29:56.874527Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:30:09.084987Z","iopub.status.busy":"2023-05-11T02:30:09.084485Z","iopub.status.idle":"2023-05-11T02:30:12.624214Z","shell.execute_reply":"2023-05-11T02:30:12.623445Z","shell.execute_reply.started":"2023-05-11T02:30:09.084944Z"},"trusted":true},"outputs":[],"source":["epoch = 5\n","input_shape = train_feature.shape[1]\n","\n","inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","x = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n","x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n","x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n","x = layers.Bidirectional(layers.LSTM(64))(x)\n","x = layers.Dense(64, activation=\"relu\")(x) \n","x = layers.Dense(16, activation=\"relu\")(x) \n","x = layers.Dense(8, activation=\"relu\")(x) \n","outputs = layers.Dense(4, activation=\"softmax\")(x)\n","model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n","\n","\n","\n","model.compile(loss=\"categorical_crossentropy\",\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=[\"accuracy\"])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:30:13.874119Z","iopub.status.busy":"2023-05-11T02:30:13.873726Z","iopub.status.idle":"2023-05-11T02:30:13.880022Z","shell.execute_reply":"2023-05-11T02:30:13.879052Z","shell.execute_reply.started":"2023-05-11T02:30:13.874084Z"},"trusted":true},"outputs":[],"source":["# Callbacks\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n","                                                  patience=5) # if val loss decreases for 3 epochs in a row, stop training\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n","                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n","                                                 patience=3,\n","                                                 verbose=1, # print out when learning rate goes down \n","                                                 min_lr=1e-7)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:30:14.872404Z","iopub.status.busy":"2023-05-11T02:30:14.872041Z","iopub.status.idle":"2023-05-11T02:30:18.055703Z","shell.execute_reply":"2023-05-11T02:30:18.054790Z","shell.execute_reply.started":"2023-05-11T02:30:14.872376Z"},"trusted":true},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_tensor_slices((train_feature, train_label))\n","train_dataset =  train_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n","\n","valid_dataset = tf.data.Dataset.from_tensor_slices((valid_feature, valid_label))\n","valid_dataset = valid_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_feature, test_label))\n","test_dataset = test_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n","\n","train_dataset, valid_dataset, test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T02:30:28.972217Z","iopub.status.busy":"2023-05-11T02:30:28.971830Z"},"trusted":true},"outputs":[],"source":["# fit the model\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","tf.get_logger().setLevel('ERROR')\n","tf.autograph.set_verbosity(1)\n","tf.set_seed = 42\n","\n","start = datetime.now()\n","with mlflow.start_run():\n","    mlflow.tensorflow.autolog()\n","    \n","    # fit the model\n","    # with tf.device('/GPU:0'):\n","    start = datetime.now()\n","    history_model = model.fit(train_dataset,\n","                            batch_size=2048,\n","                            steps_per_epoch=len(train_dataset),\n","                            validation_data=valid_dataset,\n","                            validation_steps=int(len(valid_dataset)),\n","                            callbacks=[early_stopping, reduce_lr],\n","                            epochs=epoch) \n","    end = datetime.now()\n","    print(f\"The time taken to train the model is {end - start}\")\n","        \n","    # Evaluate model\n","    model.evaluate(test_dataset)\n","    \n","    mlflow.log_param(\"Model\"   , \"LSTM\")\n","    mlflow.log_param(\"Dataset\" , f'{load_data_path}')\n","    # mlflow.log_params(model_results)\n","    mlflow.tensorflow.autolog()\n","    \n","    # Calculate the metrics\n","    model_preds_probs   = model.predict(test_feature)\n","    av_precision    = average_precision_score(test_label, model_preds_probs)\n","    print(av_precision)\n","    mlflow.log_metric(\"av_precision\"   , av_precision)\n","\n","print(\"________________________________________\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build the model \n","input_shape = 4\n","\n","inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","x = layers.Dense(32, activation ='relu')(inputs)\n","x = layers.Dense(64, activation = 'relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(128, activation ='relu')(x)\n","x = layers.Dense(512, activation = 'relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.25)(x)\n","x = layers.Dense(1024, activation ='relu')(x)\n","x = layers.Dense(512, activation = 'relu')(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dropout(0.5)(x)\n","x = layers.Dense(128, activation=\"relu\")(x)\n","x = layers.Dense(64, activation=\"relu\")(x)\n","x = layers.Dense(32, activation=\"relu\")(x)\n","outputs = layers.Dense(4, activation=\"softmax\",name=\"output_layer\")(x)      \n","model_DNN = tf.keras.Model(inputs, outputs) \n","\n","model_DNN.compile(loss=\"categorical_crossentropy\",\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=[\"accuracy\"])\n","\n","model_DNN.summary()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit the model\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","tf.get_logger().setLevel('ERROR')\n","tf.autograph.set_verbosity(1)\n","tf.set_seed = 42\n","np.random.seed(40)\n","counter = 1\n","epoch = 5\n","Model_Name = 'DNN_model'\n","\n","\n","with mlflow.start_run():\n","    mlflow.tensorflow.autolog()\n","    \n","    # fit the model\n","    # with tf.device('/GPU:0'):\n","    start = datetime.now()\n","    history_model = model_DNN.fit(train_dataset,\n","                                  batch_size=2048,\n","                                  steps_per_epoch=len(train_dataset),\n","                                  validation_data=valid_dataset,\n","                                  validation_steps=int(len(valid_dataset)),\n","                                  callbacks=[early_stopping, reduce_lr],\n","                                  epochs=epoch) \n","    end = datetime.now()\n","    print(f\"The time taken to train the model is {end - start}\")\n","        \n","    # Evaluate model\n","    model_DNN.evaluate(test_dataset)\n","    \n","    # Calculate the metrics\n","    # model_results = eval_metrics(model, \n","    #                              test_features = test_feature,\n","    #                              test_labels = test_label)\n","    \n","    mlflow.log_param(\"Model\"           , Model_Name)\n","    mlflow.log_param(\"Dataset\" , f'{load_data_path}')\n","    # mlflow.log_params(model_results)\n","    mlflow.tensorflow.autolog()\n","    \n","    # Calculate the metrics\n","    model_preds_probs   = model_DNN.predict(test_feature)\n","    av_precision    = average_precision_score(test_label, model_preds_probs)\n","    print(av_precision)\n","    mlflow.log_metric(\"av_precision\"   , av_precision)\n","    \n","    \n","    print(\"________________________________________\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
