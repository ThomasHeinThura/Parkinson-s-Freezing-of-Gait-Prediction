{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# set_defog_dataset = pd.read_parquet('Data/Process/set_defog_data.parquet')\n",
    "# test_defog_dataset = pd.read_parquet('Data/Process/test_defog_data.parquet')\n",
    "# Test_defog_dataset = pd.read_csv('Data/test/defog/02ab235146.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing_the_dataset(df):\n",
    "#     def check_skewness(df):\n",
    "#     # this can check relation between each column\n",
    "#         skew_limit=0.75\n",
    "#         skew_value=df[df.columns].skew()\n",
    "#         #print(skew_value)\n",
    "#         skew_col=skew_value[abs(skew_value)>skew_limit]\n",
    "#         cols=skew_col.index\n",
    "#         return cols\n",
    "\n",
    "#     import random \n",
    "#     random_seed = 54\n",
    "    \n",
    "#     feature_col = ['AccV','AccML','AccAP']\n",
    "#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "\n",
    "#     # make feature and label\n",
    "#     feature_dataset = df[feature_col]\n",
    "#     label_dataset = df[label_col]\n",
    "    \n",
    "#     # check skewness and powertransform\n",
    "#     skew_columns = check_skewness(feature_dataset)\n",
    "#     print(skew_columns)\n",
    "    \n",
    "#     print(\"Power Transform start\")\n",
    "#     from sklearn.preprocessing import PowerTransformer\n",
    "#     pt=PowerTransformer(standardize=False)  \n",
    "#     feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n",
    "    \n",
    "#     print(\"Standardization start\")\n",
    "#     # Change features data to 0 and 1\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     sc=StandardScaler()\n",
    "#     feature_dataset=sc.fit_transform(feature_dataset)\n",
    "    \n",
    "#     print(\"Train test split begin\")\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n",
    "    \n",
    "#     train_feature = np.array(train_feature) \n",
    "#     valid_feature = np.array(valid_feature)\n",
    "#     train_label  = np.array(train_label)\n",
    "#     valid_label = np.array(valid_label)\n",
    "    \n",
    "#     return train_feature, valid_feature, train_label, valid_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_defog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Data/defog/train_feature',train_feature)\n",
    "# np.save('Data/defog/valid_feature',valid_feature)\n",
    "# np.save('Data/defog/train_label',train_label)\n",
    "# np.save('Data/defog/valid_label',valid_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bz of low memory, I save the features and label then load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "train_feature = np.load('Data/defog/train_feature.npy')\n",
    "valid_feature = np.load('Data/defog/valid_feature.npy')\n",
    "train_label = np.load('Data/defog/train_label.npy')\n",
    "valid_label = np.load('Data/defog/valid_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29126467, 3), (7281617, 3), (29126467, 4), (7281617, 4))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn model\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "\n",
    "# import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Building Model Dict\n",
    "Models = {                         \n",
    "    # \"Decision Tree\": DecisionTreeClassifier(),      \n",
    "    \"KNearest\": KNeighborsClassifier(n_jobs=-1),           \n",
    "    # \"Ridge\" : RidgeClassifier(),              #poor result    \n",
    "    # \"MLP\" : MLPClassifier(),                  #poor result              \n",
    "    # \"R_Neighour\" : RadiusNeighborsClassifier(),\n",
    "    # \"Extra_T\" : ExtraTreesClassifier(),\n",
    "    # \"R_forest\" : RandomForestClassifier(),\n",
    "    # \"XGB\" : xgb.XGBClassifier(),\n",
    "    # \"Catboost\" : CatBoostClassifier()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGB\n",
    "\n",
    "# lgb_train = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1})\n",
    "# lgb_test = lgb.Dataset(train_nm_features, train_nm_labels,  params={'verbose': -1}, reference=lgb_train)\n",
    "\n",
    "# LGBM = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_test], verbose_eval=False)\n",
    "# LGBM_pred=LGBM.predict(test_nm_features)\n",
    "# LGBM_pred = LGBM_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# !mlflow server --backend-store-uri sqlite:///backend.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(classifier, test_features, test_labels):\n",
    "    \n",
    "    # make prediction\n",
    "    predictions   = classifier.predict(test_features)\n",
    "    \n",
    "    base_score   = classifier.score(test_features,test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    av_precision = average_precision_score(test_labels, predictions)\n",
    "\n",
    "    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "    print(\"Classification report\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(classification_report(test_labels, predictions,target_names=target_names),\"\\n\")\n",
    "\n",
    "    print(\"Accuracy Measures\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(\"Base score: \", base_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Avarge Precision: \", av_precision)\n",
    "    \n",
    "    return base_score,accuracy,av_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://127.0.0.1:5000'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. KNearest\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       1.00      1.00      1.00   1822100\n",
      "           Turn       0.84      0.93      0.89   1819501\n",
      "        Walking       0.89      0.98      0.93   1819885\n",
      "       All_zero       0.94      0.74      0.83   1820131\n",
      "\n",
      "      micro avg       0.92      0.91      0.91   7281617\n",
      "      macro avg       0.92      0.91      0.91   7281617\n",
      "   weighted avg       0.92      0.91      0.91   7281617\n",
      "    samples avg       0.91      0.91      0.91   7281617\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9114667799748325\n",
      "Accuracy:  0.9114667799748325\n",
      "Avarge Precision:  0.85981436472155\n",
      "Because f1 socre is not quality. The model is skip to saving phase.\n",
      "________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/miniconda3/envs/main/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "counter = 1\n",
    "for Model_Name, classifier in Models.items(): \n",
    "    # with mlflow.start_run(nested=True):\n",
    "    print(f\"{counter}. {Model_Name}\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # fit the model\n",
    "        from joblib import parallel_backend\n",
    "\n",
    "        with parallel_backend('threading', n_jobs=-1):\n",
    "            classifier.fit(train_feature, train_label)\n",
    "        # cross_val_scores = cross_val_score(classifier, train_feature, train_label, cv=3)\n",
    "        # round_scores = round(cross_val_scores.mean(), 4) * 100\n",
    "        # print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round_scores , \"% accuracy score\")\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        base_score,accuracy,av_precision = eval_metrics(classifier,\n",
    "                                                        valid_feature,\n",
    "                                                        valid_label)  \n",
    "        \n",
    "        mlflow.log_param(\"Model\"           , Model_Name)\n",
    "        mlflow.log_param(\"Dataset\" , \"defog\")\n",
    "        mlflow.log_metric(\"base_score\"     , base_score)\n",
    "        mlflow.log_metric(\"accuracy\"       , accuracy)\n",
    "        mlflow.log_metric(\"av_precision\"   , av_precision)\n",
    "        # mlflow.log_metric(\"round_score\", round_scores)\n",
    "        \n",
    "        # signature = infer_signature(valid_feature, classifier.predict(valid_feature))\n",
    "        if av_precision > 0.95 :\n",
    "            mlflow.sklearn.log_model(classifier,Model_Name, signature=signature)\n",
    "            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n",
    "        else :\n",
    "            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n",
    "        \n",
    "        print(\"________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"Decision Tree\": {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "#               \"min_samples_leaf\": list(range(5,7,1))},\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using Grid search for finding the most optimal hyperparameters\n",
    "# def gridsearch(classifier, params):\n",
    "#     grid_classifier = GridSearchCV(classifier, params)\n",
    "#     grid_classifier.fit(train_feature, train_label)\n",
    "#     best_classifier = grid_classifier.best_estimator_\n",
    "#     return best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Cross-Validation Scores after applying GridSearch:-')\n",
    "# for key, classifier in Models.items():\n",
    "#     best_classifier = gridsearch(classifier,params[key])\n",
    "#     cv_score = cross_val_score(classifier, train_feature, train_label, cv=3)\n",
    "#     print('{}: {}'.format(key,round(cv_score.mean()*100.0, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = np.load('Data/defog/test_feature.npy')\n",
    "test_label = np.load('Data/defog/test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_score,accuracy,av_precision = eval_metrics(best_classifier,\n",
    "#                                                 test_feature,\n",
    "#                                                 test_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=-1)\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       1.00      1.00      1.00   2276249\n",
      "           Turn       0.84      0.93      0.89   2273938\n",
      "        Walking       0.89      0.98      0.93   2274579\n",
      "       All_zero       0.94      0.74      0.83   2277255\n",
      "\n",
      "      micro avg       0.91      0.91      0.91   9102021\n",
      "      macro avg       0.92      0.91      0.91   9102021\n",
      "   weighted avg       0.92      0.91      0.91   9102021\n",
      "    samples avg       0.91      0.91      0.91   9102021\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.9106806059884942\n",
      "Accuracy:  0.9106806059884942\n",
      "Avarge Precision:  0.858632248590954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/miniconda3/envs/main/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for Model_Name, classifier in Models.items(): \n",
    "    print(classifier)\n",
    "    base_score,accuracy,av_precision = eval_metrics(classifier=classifier,\n",
    "                                                    test_features=test_feature,\n",
    "                                                    test_labels= test_label)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
