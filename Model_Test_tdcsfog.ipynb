{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# set_tdcsfog_dataset = pd.read_parquet('Data/Process/set_tdcsfog_data.parquet')\n",
    "# test_tdcsfog_dataset = pd.read_parquet('Data/Process/test_tdcsfog_data.parquet')\n",
    "# Test_tdcsfog_dataset = pd.read_csv('Data/test/tdcsfog/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing_the_dataset(df):\n",
    "#     def check_skewness(df):\n",
    "#     # this can check relation between each column\n",
    "#         skew_limit=0.75\n",
    "#         skew_value=df[df.columns].skew()\n",
    "#         #print(skew_value)\n",
    "#         skew_col=skew_value[abs(skew_value)>skew_limit]\n",
    "#         cols=skew_col.index\n",
    "#         return cols\n",
    "\n",
    "#     import random \n",
    "#     random_seed = 54\n",
    "    \n",
    "#     feature_col = ['AccV','AccML','AccAP']\n",
    "#     label_col = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "\n",
    "#     # make feature and label\n",
    "#     feature_dataset = df[feature_col]\n",
    "#     label_dataset = df[label_col]\n",
    "    \n",
    "#     # check skewness and powertransform\n",
    "#     skew_columns = check_skewness(feature_dataset)\n",
    "#     print(skew_columns)\n",
    "    \n",
    "#     print(\"Power Transform start\")\n",
    "#     from sklearn.preprocessing import PowerTransformer\n",
    "#     pt=PowerTransformer(standardize=False)  \n",
    "#     feature_dataset[skew_columns] = pt.fit_transform(feature_dataset[skew_columns])\n",
    "    \n",
    "#     print(\"Standardization start\")\n",
    "#     # Change features data to 0 and 1\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     sc=StandardScaler()\n",
    "#     feature_dataset=sc.fit_transform(feature_dataset)\n",
    "    \n",
    "#     print(\"Train test split begin\")\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     train_feature, valid_feature, train_label, valid_label = train_test_split(feature_dataset, label_dataset, test_size=0.2, random_state=True)\n",
    "    \n",
    "#     train_feature = np.array(train_feature) \n",
    "#     valid_feature = np.array(valid_feature)\n",
    "#     train_label  = np.array(train_label)\n",
    "#     valid_label = np.array(valid_label)\n",
    "    \n",
    "#     return train_feature, valid_feature, train_label, valid_label\n",
    "#     # return feature_dataset, label_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature, valid_feature, train_label, valid_label = preprocessing_the_dataset(set_tdcsfog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feature, test_label = preprocessing_the_dataset(test_tdcsfog_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feature.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Data/tdcsfog/train_feature',train_feature)\n",
    "# np.save('Data/tdcsfog/valid_feature',valid_feature)\n",
    "# np.save('Data/tdcsfog/train_label',train_label)\n",
    "# np.save('Data/tdcsfog/valid_label',valid_label)\n",
    "# np.save('Data/tdcsfog/test_feature',test_feature)\n",
    "# np.save('Data/tdcsfog/test_label',test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bz of low memory, I save the features and label then load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "train_feature = np.load('Data/tdcsfog/train_feature.npy')\n",
    "valid_feature = np.load('Data/tdcsfog/valid_feature.npy')\n",
    "train_label = np.load('Data/tdcsfog/train_label.npy')\n",
    "valid_label = np.load('Data/tdcsfog/valid_label.npy')\n",
    "test_feature = np.load('Data/tdcsfog/test_feature.npy')\n",
    "test_label = np.load('Data/tdcsfog/test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12454643, 3), (3113661, 3), (12454643, 4), (3113661, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape, valid_feature.shape, train_label.shape, valid_label.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn model\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "\n",
    "# import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Building Model Dict\n",
    "Models = {                         \n",
    "    # \"Decision Tree\": DecisionTreeClassifier(),      \n",
    "    \"KNearest\": KNeighborsClassifier(n_jobs=-1),           \n",
    "    # \"Ridge\" : RidgeClassifier(),              #poor result    \n",
    "    # \"MLP\" : MLPClassifier(),                  #poor result              \n",
    "    # \"R_Neighour\" : RadiusNeighborsClassifier(),\n",
    "    # \"Extra_T\" : ExtraTreesClassifier(n_estimators=50),\n",
    "    # \"R_forest\" : RandomForestClassifier(),\n",
    "    \"XGB\" : xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0),\n",
    "    # \"Catboost\" : CatBoostClassifier()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# !mlflow server --backend-store-uri sqlite:///backend.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(classifier, test_features, test_labels):\n",
    "    \n",
    "    # make prediction\n",
    "    predictions   = classifier.predict(test_features)\n",
    "    \n",
    "    base_score   = classifier.score(test_features,test_labels)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    av_precision = average_precision_score(test_labels, predictions)\n",
    " \n",
    "    \n",
    "    target_names = ['StartHesitation','Turn','Walking', 'All_zero']\n",
    "    print(\"Classification report\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(classification_report(test_labels, predictions,target_names=target_names),\"\\n\")\n",
    "    \n",
    "    print(\"Accuracy Measures\")\n",
    "    print(\"---------------------\",\"\\n\")\n",
    "    print(\"Base score: \", base_score)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Avarge Precision: \", av_precision)\n",
    "    \n",
    "    return base_score,accuracy,av_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking URI: 'http://127.0.0.1:5000'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "print(f\"tracking URI: '{mlflow.get_tracking_uri()}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "counter = 1\n",
    "for Model_Name, classifier in Models.items(): \n",
    "    # with mlflow.start_run(nested=True):\n",
    "    print(f\"{counter}. {Model_Name}\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # fit the model\n",
    "        from joblib import parallel_backend\n",
    "\n",
    "        with parallel_backend('threading', n_jobs=-1):\n",
    "            classifier.fit(train_feature, train_label)\n",
    "        # cross_val_scores = cross_val_score(classifier, train_feature, train_label, cv=3)\n",
    "        # round_scores = round(cross_val_scores.mean(), 4) * 100\n",
    "        # print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round_scores , \"% accuracy score\")\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        base_score,accuracy,av_precision = eval_metrics(classifier,\n",
    "                                                        valid_feature,\n",
    "                                                        valid_label)  \n",
    "        \n",
    "        mlflow.log_param(\"Model\"           , Model_Name)\n",
    "        mlflow.log_param(\"Dataset\" , \"Tdcsfog\")\n",
    "        mlflow.log_metric(\"base_score\"     , base_score)\n",
    "        mlflow.log_metric(\"accuracy\"       , accuracy)\n",
    "        mlflow.log_metric(\"av_precision\"   , av_precision)\n",
    "        # mlflow.log_metric(\"round_score\", round_scores)\n",
    "        \n",
    "        # signature = infer_signature(valid_feature, classifier.predict(valid_feature))\n",
    "        if av_precision > 0.95 :\n",
    "            mlflow.sklearn.log_model(classifier,Model_Name)\n",
    "            print(f\"f1 socre is more than 0.945 so the {Model_Name} is saved\")\n",
    "        else :\n",
    "            print(f\"Because f1 socre is not quality. The model is skip to saving phase.\")\n",
    "        \n",
    "        print(\"________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LGB\n",
    "from sklearn.metrics import f1_score\n",
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'multiclass',\n",
    "          'metric':'multi_logloss',\n",
    "          'num_class':4,\n",
    "          'learning_rate': 0.05,\n",
    "          'verbose' : -1\n",
    "         }\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(train_feature, label = train_label)\n",
    "lgb_test = lgb.Dataset(valid_feature, label = valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM = lgb.train(params, lgb_train, 100)\n",
    "LGBM_pred=LGBM.predict(test_feature)\n",
    "LGBM_pred = LGBM_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25020785822270686 0.24999999999999997\n"
     ]
    }
   ],
   "source": [
    "LGBM_accuracy = accuracy_score(y_true=test_label,\n",
    "                               y_pred=LGBM_pred,\n",
    "                            #    average='weighted'\n",
    "                            )\n",
    "LGBM_av_precision = average_precision_score(\n",
    "   y_true=test_label,\n",
    "   y_score=LGBM_pred\n",
    ")\n",
    "print(LGBM_accuracy, LGBM_av_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost=CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42,)\n",
    "# Catboost.fit(train_feature,train_label,eval_set=(valid_feature,valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cat_pred = Catboost.predict(test_feature)\n",
    "# base_score, accuracy, av_precision = eval_metrics(classifier = Castboost,\n",
    "#                                       test_feature = test_feature,\n",
    "#                                       test_label = test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=-1)\n",
      "Classification report\n",
      "--------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       0.80      0.87      0.83    973828\n",
      "           Turn       0.71      0.65      0.68    973227\n",
      "        Walking       0.81      0.88      0.84    973253\n",
      "       All_zero       0.80      0.54      0.65    971768\n",
      "\n",
      "      micro avg       0.78      0.74      0.76   3892076\n",
      "      macro avg       0.78      0.74      0.75   3892076\n",
      "   weighted avg       0.78      0.74      0.75   3892076\n",
      "    samples avg       0.74      0.74      0.74   3892076\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.7369912612189484\n",
      "Accuracy:  0.7369912612189484\n",
      "Avarge Precision:  0.6422217471842908\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=0, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "Classification report\n",
      "--------------------- \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "StartHesitation       0.61      0.13      0.21    973828\n",
      "           Turn       0.66      0.12      0.20    973227\n",
      "        Walking       0.59      0.20      0.30    973253\n",
      "       All_zero       0.75      0.42      0.54    971768\n",
      "\n",
      "      micro avg       0.67      0.22      0.33   3892076\n",
      "      macro avg       0.65      0.22      0.31   3892076\n",
      "   weighted avg       0.65      0.22      0.31   3892076\n",
      "    samples avg       0.22      0.22      0.22   3892076\n",
      " \n",
      "\n",
      "Accuracy Measures\n",
      "--------------------- \n",
      "\n",
      "Base score:  0.2157064764408506\n",
      "Accuracy:  0.2157064764408506\n",
      "Avarge Precision:  0.3425824317761779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanlinn/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for Model_Name, classifier in Models.items(): \n",
    "    print(classifier)\n",
    "    base_score,accuracy,av_precision = eval_metrics(classifier=classifier,\n",
    "                                                    test_features=test_feature,\n",
    "                                                    test_labels= test_label)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
